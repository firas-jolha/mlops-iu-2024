<!DOCTYPE html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="mobile-web-app-capable" content="yes">
    <title>
        Phase II - Data preparation/engineering - CodiMD
    </title>
    <link rel="icon" type="image/png" href="http://localhost:3000/favicon.png">
    <link rel="apple-touch-icon" href="http://localhost:3000/apple-touch-icon.png">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.0/css/bootstrap.min.css" integrity="sha256-H0KfTigpUV+0/5tn2HXC0CPwhhDhWgSawJdnFd0CGCo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fork-awesome/1.1.3/css/fork-awesome.min.css" integrity="sha256-ZhApazu+kejqTYhMF+1DzNKjIzP7KXu6AzyXcC1gMus=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/ionicons/2.0.1/css/ionicons.min.css" integrity="sha256-3iu9jgsy9TpTwXKb7bNQzqWekRX7pPK+2OLj3R922fo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" integrity="sha256-vtR0hSWRc3Tb26iuN2oZHt3KRUomwTufNIf5/4oeCyg=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github-gist.min.css" integrity="sha256-tAflq+ymku3Khs+I/WcAneIlafYgDiOQ9stIHH985Wo=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@hackmd/emojify.js@2.1.0/dist/css/basic/emojify.min.css" integrity="sha256-UOrvMOsSDSrW6szVLe8ZDZezBxh5IoIfgTwdNDgTjiU=" crossorigin="anonymous" />
    <style>
        @import url(https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400italic,600,600italic,300italic,300|Source+Serif+Pro|Source+Code+Pro:400,300,500&subset=latin,latin-ext);.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;font-size:16px;line-height:1.5;word-wrap:break-word}.markdown-body:after,.markdown-body:before{display:table;content:""}.markdown-body:after{clear:both}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body .absent{color:#c00}.markdown-body .anchor{float:left;padding-right:4px;margin-left:-20px;line-height:1}.markdown-body .anchor:focus{outline:none}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body hr{height:.25em;padding:0;margin:24px 0;background-color:#e7e7e7;border:0}.markdown-body blockquote{padding:0 1em;color:#777;border-left:.25em solid #ddd}.night .markdown-body blockquote{color:#bcbcbc}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body .loweralpha{list-style-type:lower-alpha}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.night .markdown-body h1,.night .markdown-body h2,.night .markdown-body h3,.night .markdown-body h4,.night .markdown-body h5,.night .markdown-body h6{color:#ddd}.markdown-body h1 .fa-link,.markdown-body h2 .fa-link,.markdown-body h3 .fa-link,.markdown-body h4 .fa-link,.markdown-body h5 .fa-link,.markdown-body h6 .fa-link{color:#000;vertical-align:middle;visibility:hidden;font-size:16px}.night .markdown-body h1 .fa-link,.night .markdown-body h2 .fa-link,.night .markdown-body h3 .fa-link,.night .markdown-body h4 .fa-link,.night .markdown-body h5 .fa-link,.night .markdown-body h6 .fa-link{color:#fff}.markdown-body h1:hover .anchor,.markdown-body h2:hover .anchor,.markdown-body h3:hover .anchor,.markdown-body h4:hover .anchor,.markdown-body h5:hover .anchor,.markdown-body h6:hover .anchor{text-decoration:none}.markdown-body h1:hover .anchor .fa-link,.markdown-body h2:hover .anchor .fa-link,.markdown-body h3:hover .anchor .fa-link,.markdown-body h4:hover .anchor .fa-link,.markdown-body h5:hover .anchor .fa-link,.markdown-body h6:hover .anchor .fa-link{visibility:visible}.markdown-body h1 code,.markdown-body h1 tt,.markdown-body h2 code,.markdown-body h2 tt,.markdown-body h3 code,.markdown-body h3 tt,.markdown-body h4 code,.markdown-body h4 tt,.markdown-body h5 code,.markdown-body h5 tt,.markdown-body h6 code,.markdown-body h6 tt{font-size:inherit}.markdown-body h1{font-size:2em}.markdown-body h1,.markdown-body h2{padding-bottom:.3em;border-bottom:1px solid #eee}.markdown-body h2{font-size:1.5em}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#777}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol.no-list,.markdown-body ul.no-list{padding:0;list-style-type:none}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body table{display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}.markdown-body table th{font-weight:700}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #ddd}.markdown-body table tr{background-color:#fff;border-top:1px solid #ccc}.night .markdown-body table tr{background-color:#5f5f5f}.markdown-body table tr:nth-child(2n){background-color:#f8f8f8}.night .markdown-body table tr:nth-child(2n){background-color:#4f4f4f}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body img[align=right]{padding-left:20px}.markdown-body img[align=left]{padding-right:20px}.markdown-body .emoji{max-width:none;vertical-align:text-top;background-color:transparent}.markdown-body span.frame{display:block;overflow:hidden}.markdown-body span.frame>span{display:block;float:left;width:auto;padding:7px;margin:13px 0 0;overflow:hidden;border:1px solid #ddd}.markdown-body span.frame span img{display:block;float:left}.markdown-body span.frame span span{display:block;padding:5px 0 0;clear:both;color:#333}.markdown-body span.align-center{display:block;overflow:hidden;clear:both}.markdown-body span.align-center>span{display:block;margin:13px auto 0;overflow:hidden;text-align:center}.markdown-body span.align-center span img{margin:0 auto;text-align:center}.markdown-body span.align-right{display:block;overflow:hidden;clear:both}.markdown-body span.align-right>span{display:block;margin:13px 0 0;overflow:hidden;text-align:right}.markdown-body span.align-right span img{margin:0;text-align:right}.markdown-body span.float-left{display:block;float:left;margin-right:13px;overflow:hidden}.markdown-body span.float-left span{margin:13px 0 0}.markdown-body span.float-right{display:block;float:right;margin-left:13px;overflow:hidden}.markdown-body span.float-right>span{display:block;margin:13px auto 0;overflow:hidden;text-align:right}.markdown-body code,.markdown-body tt{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(0,0,0,.04);border-radius:3px}.night .markdown-body code,.night .markdown-body tt{color:#eee;background-color:hsla(0,0%,90.2%,.36)}.markdown-body code:after,.markdown-body code:before,.markdown-body tt:after,.markdown-body tt:before{letter-spacing:-.2em;content:"\A0"}.markdown-body code br,.markdown-body tt br{display:none}.markdown-body del code{text-decoration:inherit}.markdown-body pre{word-wrap:normal}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:transparent;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f7f7f7;border-radius:3px}.markdown-body pre code,.markdown-body pre tt{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code:after,.markdown-body pre code:before,.markdown-body pre tt:after,.markdown-body pre tt:before{content:normal}.markdown-body .csv-data td,.markdown-body .csv-data th{padding:5px;overflow:hidden;font-size:12px;line-height:1;text-align:left;white-space:nowrap}.markdown-body .csv-data .blob-line-num{padding:10px 8px 9px;text-align:right;background:#fff;border:0}.markdown-body .csv-data tr{border-top:0}.markdown-body .csv-data th{font-weight:700;background:#f8f8f8;border-top:0}.markdown-body kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#555;vertical-align:middle;background-color:#fcfcfc;border:1px solid;border-color:#ccc #ccc #bbb;border-radius:3px;box-shadow:inset 0 -1px 0 #bbb}.news .alert .markdown-body blockquote{padding:0 0 0 40px;border:0}.activity-tab .news .alert .commits,.activity-tab .news .markdown-body blockquote{padding-left:0}.task-list-item{list-style-type:none}.task-list-item label{font-weight:400}.task-list-item.enabled label{cursor:pointer}.task-list-item+.task-list-item{margin-top:3px}.task-list-item-checkbox{float:left;margin:.31em 0 .2em -1.3em!important;vertical-align:middle;cursor:default!important}.markdown-body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol;padding-top:40px;padding-bottom:40px;max-width:758px;overflow:visible!important}.markdown-body pre{border:inherit!important}.night .markdown-body pre{filter:invert(100%)}.markdown-body code{color:inherit!important}.markdown-body pre code .wrapper{display:-webkit-inline-flex;display:-moz-inline-flex;display:-ms-inline-flex;display:-o-inline-flex;display:inline-flex}.markdown-body pre code .gutter{float:left;overflow:hidden;-webkit-user-select:none;user-select:none}.markdown-body pre code .gutter.linenumber{text-align:right;position:relative;display:inline-block;cursor:default;z-index:4;padding:0 8px 0 0;min-width:20px;box-sizing:content-box;color:#afafaf!important;border-right:3px solid #6ce26c!important}.markdown-body pre code .gutter.linenumber>span:before{content:attr(data-linenumber)}.markdown-body pre code .code{float:left;margin:0 0 0 16px}.markdown-body .gist .line-numbers{border-left:none;border-top:none;border-bottom:none}.markdown-body .gist .line-data{border:none}.markdown-body .gist table{border-spacing:0;border-collapse:inherit!important}.night .markdown-body .gist table tr:nth-child(2n){background-color:#ddd}.markdown-body code[data-gist-id]{background:none;padding:0;filter:invert(100%)}.markdown-body code[data-gist-id]:after,.markdown-body code[data-gist-id]:before{content:""}.markdown-body code[data-gist-id] .blob-num{border:unset}.markdown-body code[data-gist-id] table{overflow:unset;margin-bottom:unset}.markdown-body code[data-gist-id] table tr{background:unset}.markdown-body[dir=rtl] pre{direction:ltr}.markdown-body[dir=rtl] code{direction:ltr;unicode-bidi:embed}.markdown-body .alert>p{margin-bottom:0}.markdown-body pre.abc,.markdown-body pre.flow-chart,.markdown-body pre.geo,.markdown-body pre.graphviz,.markdown-body pre.mermaid,.markdown-body pre.sequence-diagram,.markdown-body pre.vega{text-align:center;background-color:inherit;border-radius:0;white-space:inherit}.night .markdown-body pre.graphviz .graph>polygon{fill:#333}.night .markdown-body pre.mermaid .sectionTitle,.night .markdown-body pre.mermaid .titleText,.night .markdown-body pre.mermaid text{fill:#fff}.markdown-body pre.abc>code,.markdown-body pre.flow-chart>code,.markdown-body pre.graphviz>code,.markdown-body pre.mermaid>code,.markdown-body pre.sequence-diagram>code,.markdown-body pre.vega>code{text-align:left}.markdown-body pre.abc>svg,.markdown-body pre.flow-chart>svg,.markdown-body pre.graphviz>svg,.markdown-body pre.mermaid>svg,.markdown-body pre.sequence-diagram>svg,.markdown-body pre.vega>svg{max-width:100%;height:100%}.night .markdown-body .abc path{fill:#eee}.night .markdown-body .abc path.note_selected{fill:##4DD0E1}.night tspan{fill:#fefefe}.night pre rect{fill:transparent}.night pre.flow-chart path,.night pre.flow-chart rect{stroke:#fff}.markdown-body pre>code.wrap{white-space:pre-wrap;white-space:-moz-pre-wrap;white-space:-pre-wrap;white-space:-o-pre-wrap;word-wrap:break-word}.markdown-body .alert>p,.markdown-body .alert>ul{margin-bottom:0}.markdown-body summary{display:list-item}.markdown-body summary:focus{outline:none}.markdown-body details summary{cursor:pointer}.markdown-body details:not([open])>:not(summary){display:none}.markdown-body figure{margin:1em 40px}.markdown-body img{background-color:transparent}.vimeo,.youtube{cursor:pointer;display:table;text-align:center;background-position:50%;background-repeat:no-repeat;background-size:contain;background-color:#000;overflow:hidden}.vimeo,.youtube{position:relative;width:100%}.youtube{padding-bottom:56.25%}.vimeo img{width:100%;object-fit:contain;z-index:0}.youtube img{object-fit:cover;z-index:0}.vimeo iframe,.youtube iframe,.youtube img{width:100%;height:100%;position:absolute;top:0;left:0}.vimeo iframe,.youtube iframe{vertical-align:middle;z-index:1}.vimeo .icon,.youtube .icon{position:absolute;height:auto;width:auto;top:50%;left:50%;transform:translate(-50%,-50%);color:#fff;opacity:.3;-webkit-transition:opacity .2s;transition:opacity .2s;z-index:0}.vimeo:hover .icon,.youtube:hover .icon{opacity:.6;-webkit-transition:opacity .2s;transition:opacity .2s}.slideshare .inner,.speakerdeck .inner{position:relative;width:100%}.slideshare .inner iframe,.speakerdeck .inner iframe{position:absolute;top:0;bottom:0;left:0;right:0;width:100%;height:100%}.geo-map{width:100%;height:250px}.markmap-container{height:300px}.markmap-container>svg{width:100%;height:100%}.MJX_Assistive_MathML{display:none}.ui-infobar{position:relative;z-index:2;max-width:758px;margin-top:25px;margin-bottom:-25px;color:#777}.toc .invisable-node{list-style-type:none}.ui-toc{position:fixed;bottom:20px;z-index:10000}.ui-toc-label{opacity:.9;background-color:#ccc;border:none}.ui-toc-label,.ui-toc .open .ui-toc-label{-webkit-transition:opacity .2s;transition:opacity .2s}.ui-toc .open .ui-toc-label{opacity:1;color:#5f5f5f}.ui-toc-label:focus{opacity:1;background-color:#ccc;color:#000}.ui-toc-label:hover{opacity:1;background-color:#ccc;-webkit-transition:opacity .2s;transition:opacity .2s}.ui-toc-dropdown{margin-top:23px;margin-bottom:20px;padding-left:10px;padding-right:10px;max-width:45vw;width:25vw;max-height:70vh;overflow:auto;text-align:inherit}.ui-toc-dropdown>.toc{max-height:calc(70vh - 100px);overflow:auto}.ui-toc-dropdown[dir=rtl] .nav{padding-right:0;letter-spacing:.0029em}.ui-toc-dropdown a{overflow:hidden;text-overflow:ellipsis;white-space:pre}.ui-toc-dropdown .nav>li>a{display:block;padding:4px 20px;font-size:13px;font-weight:500;color:#767676}.ui-toc-dropdown .nav>li:first-child:last-child>ul,.ui-toc-dropdown .toc.expand ul{display:block}.ui-toc-dropdown .nav>li>a:focus,.ui-toc-dropdown .nav>li>a:hover{padding-left:19px;color:#000;text-decoration:none;background-color:transparent;border-left:1px solid #000}.night .ui-toc-dropdown .nav>li>a:focus,.night .ui-toc-dropdown .nav>li>a:hover{color:#fff;border-left-color:#fff}.ui-toc-dropdown[dir=rtl] .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav>li>a:hover{padding-right:19px;border-left:none;border-right:1px solid #000}.ui-toc-dropdown .nav>.active:focus>a,.ui-toc-dropdown .nav>.active:hover>a,.ui-toc-dropdown .nav>.active>a{padding-left:18px;font-weight:700;color:#000;background-color:transparent;border-left:2px solid #000}.night .ui-toc-dropdown .nav>.active:focus>a,.night .ui-toc-dropdown .nav>.active:hover>a,.night .ui-toc-dropdown .nav>.active>a{color:#fff;border-left:2px solid #fff}.ui-toc-dropdown[dir=rtl] .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav>.active>a{padding-right:18px;border-left:none;border-right:2px solid #000}.ui-toc-dropdown .nav .nav{display:none;padding-bottom:10px}.ui-toc-dropdown .nav>.active>ul{display:block}.ui-toc-dropdown .nav .nav>li>a{padding-top:1px;padding-bottom:1px;padding-left:30px;font-size:12px;font-weight:400}.night .ui-toc-dropdown .nav>li>a{color:#aaa}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a{padding-right:30px}.ui-toc-dropdown .nav .nav>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:40px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a{padding-right:40px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:50px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>a{padding-right:50px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>ul>li>a{padding-top:1px;padding-bottom:1px;padding-left:60px;font-size:12px;font-weight:400}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>ul>li>a{padding-right:60px}.ui-toc-dropdown .nav .nav>li>a:focus,.ui-toc-dropdown .nav .nav>li>a:hover{padding-left:29px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>a:hover{padding-right:29px}.ui-toc-dropdown .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>a:hover{padding-left:39px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>a:hover{padding-right:39px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>a:hover{padding-left:49px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>a:hover{padding-right:49px}.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>ul>li>a:focus,.ui-toc-dropdown .nav .nav>li>ul>li>ul>li>ul>li>a:hover{padding-left:59px}.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>ul>li>a:focus,.ui-toc-dropdown[dir=rtl] .nav .nav>li>ul>li>ul>li>ul>li>a:hover{padding-right:59px}.ui-toc-dropdown .nav .nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>a{padding-left:28px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>a{padding-right:28px}.ui-toc-dropdown .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>a{padding-left:38px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active:hover>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>a{padding-right:38px}.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>a{padding-left:48px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.active>.nav>.nav>.active>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active:hover>a{padding-right:48px}.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:hover>a,.ui-toc-dropdown .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active>a{padding-left:58px;font-weight:500}.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.active>.nav>.nav>.active>.nav>.active>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:focus>a,.ui-toc-dropdown[dir=rtl] .nav .nav>.active>.nav>.active>.nav>.active>.nav>.active:hover>a{padding-right:58px}.markdown-body[lang^=ja]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,Hiragino Kaku Gothic Pro,"\30D2\30E9\30AE\30CE\89D2\30B4   Pro W3",Osaka,Meiryo,"\30E1\30A4\30EA\30AA",MS Gothic,"\FF2D\FF33   \30B4\30B7\30C3\30AF",sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}.ui-toc-dropdown[lang^=ja]{font-family:Source Sans Pro,Helvetica,Arial,Meiryo UI,MS PGothic,"\FF2D\FF33   \FF30\30B4\30B7\30C3\30AF",sans-serif}.markdown-body[lang=zh-tw]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,PingFang TC,Microsoft JhengHei,"\5FAE\8EDF\6B63\9ED1",sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}.ui-toc-dropdown[lang=zh-tw]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft JhengHei UI,"\5FAE\8EDF\6B63\9ED1UI",sans-serif}.markdown-body[lang=zh-cn]{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Helvetica,Arial,PingFang SC,Microsoft YaHei,"\5FAE\8F6F\96C5\9ED1",sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol}.ui-toc-dropdown[lang=zh-cn]{font-family:Source Sans Pro,Helvetica,Arial,Microsoft YaHei UI,"\5FAE\8F6F\96C5\9ED1UI",sans-serif}.ui-affix-toc{position:fixed;top:0;max-width:15vw;max-height:70vh;overflow:auto}.back-to-top,.expand-toggle,.go-to-bottom{display:block;padding:4px 10px;margin-top:10px;margin-left:10px;font-size:12px;font-weight:500;color:rgba(0,0,0,.85)}.back-to-top:focus,.back-to-top:hover,.expand-toggle:focus,.expand-toggle:hover,.go-to-bottom:focus,.go-to-bottom:hover{color:#563d7c;text-decoration:none}.back-to-top,.go-to-bottom{margin-top:0}.ui-user-icon{width:20px;height:20px;display:block;border-radius:3px;margin-top:2px;margin-bottom:2px;margin-right:5px;background-position:50%;background-repeat:no-repeat;background-size:contain}.ui-user-icon.small{width:18px;height:18px;display:inline-block;vertical-align:middle;margin:0 0 .2em}small span{line-height:22px}small .dropdown{display:inline-block}small .dropdown a:focus,small .dropdown a:hover{text-decoration:none}.unselectable{-moz-user-select:none;-khtml-user-select:none;-webkit-user-select:none;-o-user-select:none;user-select:none}.night .navbar{background:#333;border-bottom-color:#333;color:#eee}.night .navbar a{color:#eee}@media print{blockquote,div,img,pre,table{page-break-inside:avoid!important}a[href]:after{font-size:12px!important}}.markdown-body.slides{position:relative;z-index:1;color:#222}.markdown-body.slides:before{content:"";display:block;position:absolute;top:0;left:0;right:0;bottom:0;z-index:-1;background-color:currentColor;box-shadow:0 0 0 50vw}.markdown-body.slides section[data-markdown]{position:relative;margin-bottom:1.5em;background-color:#fff;text-align:center}.markdown-body.slides section[data-markdown] code{text-align:left}.markdown-body.slides section[data-markdown]:before{content:"";display:block;padding-bottom:56.23%}.markdown-body.slides section[data-markdown]>div:first-child{position:absolute;top:50%;left:1em;right:1em;transform:translateY(-50%);max-height:100%;overflow:hidden}.markdown-body.slides section[data-markdown]>ul{display:inline-block}.markdown-body.slides>section>section+section:after{content:"";position:absolute;top:-1.5em;right:1em;height:1.5em;border:3px solid #777}body{font-smoothing:subpixel-antialiased!important;-webkit-font-smoothing:subpixel-antialiased!important;-moz-osx-font-smoothing:auto!important;text-shadow:0 0 1em transparent,1px 1px 1.2px rgba(0,0,0,.004);-webkit-overflow-scrolling:touch;font-family:Source Sans Pro,Helvetica,Arial,sans-serif;letter-spacing:.025em}.focus,:focus{outline:none!important}::-moz-focus-inner{border:0!important}body.modal-open{overflow-y:auto;padding-right:0!important}
    </style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" integrity="sha256-3Jy/GbSLrg0o9y5Z5n1uw0qxZECH7C6OQpVBgNFYa0g=" crossorigin="anonymous"></script>
    	<script src="https://cdnjs.cloudflare.com/ajax/libs/respond.js/1.4.2/respond.min.js" integrity="sha256-g6iAfvZp+nDQ2TdTR/VVKJf3bGro4ub5fvWSWVRi2NE=" crossorigin="anonymous"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js" integrity="sha256-8E4Is26QH0bD52WoQpcB+R/tcWQtpzlCojrybUd7Mxo=" crossorigin="anonymous"></script>
    <![endif]-->
</head>

<body>
    <div id="doc" class="markdown-body container-fluid" lang="en"><h1 id="Phase-II---Data-preparationengineering"><a class="anchor hidden-xs" href="#Phase-II---Data-preparationengineering" title="Phase-II---Data-preparationengineering"><i class="fa fa-link"></i></a>Phase II - Data preparation/engineering</h1><p>Course: <strong>MLOps engineering</strong><br>
Author: <strong>Firas Jolha</strong></p><h1 id="Dataset"><a class="anchor hidden-xs" href="#Dataset" title="Dataset"><i class="fa fa-link"></i></a>Dataset</h1><ul>
<li><a href="https://archive.ics.uci.edu/static/public/222/data.csv" target="_blank" rel="noopener">Bank Marketing</a>
<ul>
<li><a href="https://archive.ics.uci.edu/dataset/222/bank+marketing" target="_blank" rel="noopener">Data description</a></li>
</ul>
</li>
</ul><h1 id="Agenda"><a class="anchor hidden-xs" href="#Agenda" title="Agenda"><i class="fa fa-link"></i></a>Agenda</h1><p></p><div class="toc"><ul>
<li><a href="#Phase-II---Data-preparationengineering" title="Phase II - Data preparation/engineering">Phase II - Data preparation/engineering</a></li>
<li><a href="#Dataset" title="Dataset">Dataset</a></li>
<li><a href="#Agenda" title="Agenda">Agenda</a></li>
<li><a href="#Description" title="Description">Description</a></li>
<li><a href="#Docker-and-Docker-compose-Extra-section-for-now" title="Docker and Docker compose [Extra section for now]">Docker and Docker compose [Extra section for now]</a><ul>
<li><a href="#Dockerfile-definition-extra-section" title="Dockerfile definition [extra section]">Dockerfile definition [extra section]</a></li>
</ul>
</li>
<li><a href="#Apache-Airflow" title="Apache Airflow">Apache Airflow</a><ul>
<li><a href="#Install-Airflow" title="Install Airflow">Install Airflow</a><ul>
<li><a href="#1-Usingpip" title="1. Usingpip">1. Usingpip</a></li>
<li><a href="#2-Using-Docker" title="2. Using Docker">2. Using Docker</a></li>
</ul>
</li>
<li><a href="#Prepare-the-workspace-tested-on-Ubuntu-2204" title="Prepare the workspace [tested on Ubuntu 22.04]">Prepare the workspace [tested on Ubuntu 22.04]</a><ul>
<li><a href="#1-Install-python311" title="1. Install python3.11">1. Install python3.11</a></li>
<li><a href="#2-Create-a-new-virtual-environment-using-Python-311" title="2. Create a new virtual environment using Python 3.11">2. Create a new virtual environment using Python 3.11</a></li>
<li><a href="#3-Create-requirementstxt-and-install-the-dependencies" title="3. Create requirements.txt and install the dependencies.">3. Create requirements.txt and install the dependencies.</a></li>
<li><a href="#4-Setup-the-Airflow-components" title="4. Setup the Airflow components">4. Setup the Airflow components</a></li>
</ul>
</li>
<li><a href="#Hello-world-workflow" title="Hello world workflow">Hello world workflow</a></li>
<li><a href="#Core-concepts" title="Core concepts">Core concepts</a></li>
<li><a href="#Airflow-components" title="Airflow components">Airflow components</a></li>
<li><a href="#DAG-Scheduling" title="DAG Scheduling">DAG Scheduling</a><ul>
<li><a href="#Scheduling-concepts" title="Scheduling concepts">Scheduling concepts</a></li>
</ul>
</li>
<li><a href="#Airflow-DAG-APIs" title="Airflow DAG APIs">Airflow DAG APIs</a></li>
<li><a href="#DAG-Examples" title="DAG Examples">DAG Examples</a><ul>
<li><a href="#1-Traditional-API-as-a-variable" title="1. Traditional API as a variable">1. Traditional API as a variable</a></li>
<li><a href="#2-Traditional-API-as-a-context" title="2. Traditional API as a context">2. Traditional API as a context</a></li>
<li><a href="#3-TaskFlow-API" title="3. TaskFlow API">3. TaskFlow API</a></li>
<li><a href="#Xcoms-extra-section" title="Xcoms [extra section]">Xcoms [extra section]</a></li>
<li><a href="#Python-tasks-with-virtual-environments" title="Python tasks with virtual environments">Python tasks with virtual environments</a></li>
<li><a href="#ExternalTaskSensor" title="ExternalTaskSensor">ExternalTaskSensor</a></li>
<li><a href="#Trigger-Dag-Run-operator" title="Trigger Dag Run operator">Trigger Dag Run operator</a></li>
</ul>
</li>
<li><a href="#Deferrable-operators-Extra-section" title="Deferrable operators [Extra section]">Deferrable operators [Extra section]</a></li>
<li><a href="#Test-a-pipeline" title="Test a pipeline">Test a pipeline</a></li>
<li><a href="#Airflow-UI" title="Airflow UI">Airflow UI</a><ul>
<li><a href="#DAGs-View" title="DAGs View">DAGs View</a></li>
<li><a href="#Cluster-Activity-View" title="Cluster Activity View">Cluster Activity View</a></li>
<li><a href="#Datasets-View" title="Datasets View">Datasets View</a></li>
<li><a href="#Grid-View" title="Grid View">Grid View</a></li>
<li><a href="#Graph-View" title="Graph View">Graph View</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#ZenML" title="ZenML">ZenML</a><ul>
<li><a href="#Core-concepts1" title="Core concepts">Core concepts</a><ul>
<li><a href="#Step" title="Step">Step</a></li>
<li><a href="#Pipelines" title="Pipelines">Pipelines</a></li>
<li><a href="#Artifacts" title="Artifacts">Artifacts</a></li>
<li><a href="#Materializers" title="Materializers">Materializers</a></li>
</ul>
</li>
<li><a href="#ZenML-Example" title="ZenML Example">ZenML Example</a><ul>
<li><a href="#ETL-data-pipeline" title="ETL data pipeline">ETL data pipeline</a></li>
</ul>
</li>
<li><a href="#Explore-ZenML-Dashboard" title="Explore ZenML Dashboard">Explore ZenML Dashboard</a></li>
<li><a href="#ZenML-Stacks" title="ZenML Stacks">ZenML Stacks</a><ul>
<li><a href="#Orchestrator" title="Orchestrator">Orchestrator</a></li>
<li><a href="#Artifact-store" title="Artifact store">Artifact store</a></li>
<li><a href="#Create-a-local-stack-extra-section" title="Create a local stack [extra section]">Create a local stack [extra section]</a></li>
</ul>
</li>
<li><a href="#Manage-artifacts-in-ZenML" title="Manage artifacts in ZenML">Manage artifacts in ZenML</a><ul>
<li><a href="#Adding-artifacts" title="Adding artifacts">Adding artifacts</a></li>
<li><a href="#Consuming-artifacts" title="Consuming artifacts">Consuming artifacts</a></li>
</ul>
</li>
<li><a href="#Schedule-ZenML-pipelines-using-Airflow" title="Schedule ZenML pipelines using Airflow">Schedule ZenML pipelines using Airflow</a></li>
</ul>
</li>
<li><a href="#Feast-Extra-section" title="Feast [Extra section]">Feast [Extra section]</a></li>
<li><a href="#Project-tasks" title="Project tasks">Project tasks</a><ul>
<li><a href="#A-Repository" title="A. Repository">A. Repository</a></li>
<li><a href="#B-Report-Only-for-Master’s-students" title="B. Report [Only for Master’s students]">B. Report [Only for Master’s students]</a></li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><p></p><h1 id="Description"><a class="anchor hidden-xs" href="#Description" title="Description"><i class="fa fa-link"></i></a>Description</h1><p>The second phase of the CRISP-ML process model aims to prepare data for the following modeling phase. Data selection, data cleaning, feature engineering, and data standardization tasks are performed during this phase.</p><p>We identify valuable and necessary features for future model training by using either filter methods, wrapper methods, or embedded methods for data selection. Furthermore, we select data by discarding samples that do not satisfy data quality requirements. At this point, we also might tackle the problem of unbalanced classes by applying over-sampling or under-sampling strategies.</p><p>The data cleaning task implies that we perform error detection and error correction steps for the available data. Adding unit testing for data will mitigate the risk of error propagation to the next phase. Depending on the machine learning task, we might need to perform feature engineering and data augmentation activities. For example, such methods include one-hot encoding, clustering, or discretization of continuous attributes.</p><p>The data standardization task denotes the process of unifying the ML tools’ input data to avoid the risk of erroneous data. Finally, the normalization task will mitigate the risk of bias to features on larger scales. We build data and input data transformation pipelines for data pre-processing and feature creation to ensure the ML application’s reproducibility during this phase.</p><div class="alert alert-warning">
<p>In the 2nd phase of the project, we will transform the data and prepare it for ML modeling. The objective here is to have automated pipelines which can start extracting the data from the data source till persisting the features in some feature store.</p>
</div><p><img src="https://i.imgur.com/RZjyhY0.jpeg" alt="" class="md-image md-image"></p><p>Data preparation is not a static phase and backtracking circles from later phases are necessary if, for example, the modeling phase or the deployment phase reveal erroneous data. So, we need to automate the data preparation pipelines.</p><div class="alert alert-danger">
<p><strong>Note:</strong><br>
The commands and scripts here are written considering by default that the Current Working Directory (CWD) is the project folder path. So ensure that you are there before you run commands. In my PC, I have <code>~/project</code> folder in my home directory which is the project folder path and it is also the CWD unless the directory is changed using <code>cd</code>.</p>
</div><h1 id="Docker-and-Docker-compose-Extra-section-for-now"><a class="anchor hidden-xs" href="#Docker-and-Docker-compose-Extra-section-for-now" title="Docker-and-Docker-compose-Extra-section-for-now"><i class="fa fa-link"></i></a>Docker and Docker compose [Extra section for now]</h1><div class="alert alert-danger">
<p>This section is extra for now but will be mandatory in phase 5 of CRISP-ML.</p>
</div><details>
<p>Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called <strong>containers</strong>.</p>
<p>In simpler words, <strong>Docker</strong> is a tool that allows to easily deploy the applications in a <em>sandbox</em> (called <em>containers</em>) to run on the host operating system i.e. Linux.</p>
<p>The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high <em>overhead</em> and hence enable more efficient usage of the underlying system and resources.</p>
<p>The industry standard today is to use <strong>Virtual Machines</strong> (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.</p>
<p>VMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.</p>
<p><img src="https://www.eginnovations.com/blog/wp-content/uploads/2020/12/container-vms.jpg" alt="" class="md-image md-image"></p>
<p><strong>Containers</strong> take a different approach, by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.</p>
<ul>
<li><strong>Dockerfile</strong> is a simple text file that contains a list of instructions that the Docker client calls while creating an image.</li>
</ul>
<center>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230406105935/dockerfile-2.png" alt="" width="400" height="150" class="md-image md-image"></p>
</center>
<ul>
<li><strong>Docker image</strong> is an artifact with several layers and a lightweight, compact stand-alone executable package that contains all of the components required to run a piece of software, including the code, a runtime, libraries, environment variables, and configuration files.</li>
<li><strong>Docker container</strong> is a runtime instance of an image.</li>
</ul>
<h2 id="Install-docker-and-docker-compose"><a class="anchor hidden-xs" href="#Install-docker-and-docker-compose" title="Install-docker-and-docker-compose"><i class="fa fa-link"></i></a>Install docker and docker compose</h2>
<p>The official website has a good tutorial to download and install both tools. I share here some of the tutorials:</p>
<ul>
<li><a href="https://docs.docker.com/get-docker/" target="_blank" rel="noopener">https://docs.docker.com/get-docker/</a></li>
<li><a href="https://docs.docker.com/compose/" target="_blank" rel="noopener">https://docs.docker.com/compose/</a></li>
</ul>
<p>After you install the tools, use the docker command to run <code>hello-world</code> container as follows:</p>
<pre><code>docker run hello-world
</code></pre>
<p>This command line will download the docker image <code>hello-world</code> for the first time and run a container.</p>
<div class="alert alert-danger">
<p><strong>Note:</strong> If docker is not running due to restrictions in Russia, then add a mirror to the configuration of the docker daemon (<code>daemon.json</code>). The guide is in this link <a href="https://dockerhub.timeweb.cloud/" target="_blank" rel="noopener">https://dockerhub.timeweb.cloud/</a>.</p>
<p><img src="https://i.imgur.com/ZB5kKZX.png" alt="" class="md-image md-image"></p>
</div>
</details><h2 id="Dockerfile-definition-extra-section"><a class="anchor hidden-xs" href="#Dockerfile-definition-extra-section" title="Dockerfile-definition-extra-section"><i class="fa fa-link"></i></a>Dockerfile definition [extra section]</h2><details>
<p><code>Dockerfile</code> contains the instructions to build a Docker image.</p>
<pre><code class="wrap dockerfile hljs"><span class="hljs-comment"># Represents the base image, which is the command that is executed first before any other commands.</span>
<span class="hljs-keyword">FROM</span> &lt;ImageName&gt;

<span class="hljs-comment"># used to copy the file/folders to the image while building the image. </span>
<span class="hljs-comment"># Source; is the location of the file/folders in the host  machine</span>
<span class="hljs-comment"># Destination: is the location of the file/folders in the container</span>
<span class="hljs-keyword">COPY</span><span class="bash"> &lt;Source&gt; &lt;Destination&gt;</span>

<span class="hljs-comment"># does the same as COPY. Additionally it lets you use URL location to download files and unzip files into the image</span>
<span class="hljs-keyword">ADD</span><span class="bash"> &lt;URL&gt; &lt;Destination&gt;</span>

<span class="hljs-comment"># Runs scripts and commands in the container. The execution of RUN commands will take place while you create an image on top of the prior layers (Image). It is used to install packages into container, create folders, etc</span>
<span class="hljs-keyword">RUN</span><span class="bash"> &lt;Command + ARGS&gt;</span>

<span class="hljs-comment"># allows you to set a default command which will be executed only when you run a container without specifying a command. If a Docker container runs with a command, the default command will be ignored, so it can be overridden. There can be only one CMD in the dockerfile. </span>
<span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-built_in">command</span> + args]</span>

<span class="hljs-comment"># A container that will function as an executable is configured by ENTRYPOINT. When you start the Docker container, a command or script called ENTRYPOINT is executed. It ca not be overridden.The only difference between CMD and ENTRYPOINT is CMD can be overridden and ENTRYPOINT can’t.</span>
<span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-built_in">command</span> + args]</span>

<span class="hljs-comment"># identifies the author/owner of the Dockerfile</span>
<span class="hljs-keyword">MAINTAINER</span> &lt;NAME&gt;

<span class="hljs-comment"># sets environment variables inside the container</span>
<span class="hljs-keyword">ENV</span> VAR VALUE

<span class="hljs-comment"># defines build-time variable.</span>
<span class="hljs-keyword">ARG</span> VAR VALUE

<span class="hljs-comment"># info to expose ports outside the container</span>
<span class="hljs-keyword">EXPOSE</span> PORT

<span class="hljs-comment"># info to create a directory mount point to access and store persistent data</span>
<span class="hljs-comment"># PATH here is container path</span>
<span class="hljs-keyword">VOLUME</span><span class="bash"> [PATH]</span>

<span class="hljs-comment"># sets the working directory for the instructions that follow</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> SOME_CONTAINER_PATH </span>
</code></pre>
<h2 id="Demo"><a class="anchor hidden-xs" href="#Demo" title="Demo"><i class="fa fa-link"></i></a>Demo</h2>
<p>Here we will create a simple Flask app, dockerize it and push it to Docker hub.</p>
<ul>
<li>Prepare the following files.
<ul>
<li>requirements.txt: contains the app dependencies.</li>
<li><a href="http://app.py" target="_blank" rel="noopener">app.py</a>: contains the Flask app code.</li>
<li>Dockerfile: contains the instructions to build a Docker image.</li>
</ul>
</li>
</ul>
<pre><code class="yaml hljs"><div class="wrapper"><div class="gutter linenumber"><span data-linenumber="1"></span>
<span data-linenumber="2"></span>
<span data-linenumber="3"></span></div><div class="code"><span class="hljs-comment"># requirements.txt</span>

<span class="hljs-string">flask</span>
</div></div></code></pre>
<pre><code class="python hljs"><div class="wrapper"><div class="gutter linenumber"><span data-linenumber="1"></span>
<span data-linenumber="2"></span>
<span data-linenumber="3"></span>
<span data-linenumber="4"></span>
<span data-linenumber="5"></span>
<span data-linenumber="6"></span>
<span data-linenumber="7"></span>
<span data-linenumber="8"></span>
<span data-linenumber="9"></span>
<span data-linenumber="10"></span>
<span data-linenumber="11"></span>
<span data-linenumber="12"></span>
<span data-linenumber="13"></span>
<span data-linenumber="14"></span></div><div class="code"><span class="hljs-comment"># app.py</span>

<span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask
<span class="hljs-keyword">import</span> os

app = Flask(__name__)
 
<span class="hljs-meta">@app.route("/")</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-string">"Hello World!"</span>
 
<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    port = int(os.environ.get(<span class="hljs-string">'PORT'</span>, <span class="hljs-number">5000</span>))
    app.run(debug=<span class="hljs-literal">True</span>, host=<span class="hljs-string">'0.0.0.0'</span>, port=port)
</div></div></code></pre>
<pre><code class="dockerfile hljs"><div class="wrapper"><div class="gutter linenumber"><span data-linenumber="1"></span>
<span data-linenumber="2"></span>
<span data-linenumber="3"></span>
<span data-linenumber="4"></span>
<span data-linenumber="5"></span>
<span data-linenumber="6"></span>
<span data-linenumber="7"></span>
<span data-linenumber="8"></span>
<span data-linenumber="9"></span>
<span data-linenumber="10"></span>
<span data-linenumber="11"></span>
<span data-linenumber="12"></span>
<span data-linenumber="13"></span>
<span data-linenumber="14"></span>
<span data-linenumber="15"></span>
<span data-linenumber="16"></span>
<span data-linenumber="17"></span>
<span data-linenumber="18"></span>
<span data-linenumber="19"></span>
<span data-linenumber="20"></span>
<span data-linenumber="21"></span>
<span data-linenumber="22"></span>
<span data-linenumber="23"></span>
<span data-linenumber="24"></span>
<span data-linenumber="25"></span>
<span data-linenumber="26"></span></div><div class="code"><span class="hljs-comment"># Dockerfile</span>

<span class="hljs-comment"># Base image</span>
<span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.8</span>-alpine

<span class="hljs-comment"># Switch to another directory</span>
<span class="hljs-comment"># CWD is /usr/src/app</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> /usr/src/app</span>

<span class="hljs-comment"># Copy the requirements.txt to CWD</span>
<span class="hljs-keyword">COPY</span><span class="bash"> requirements.txt ./</span>

<span class="hljs-comment"># Install the dependencies in requirements.txt</span>
<span class="hljs-keyword">RUN</span><span class="bash"> pip install --no-cache-dir -r requirements.txt</span>

<span class="hljs-comment"># Copy the code and everything in CWD of the host to CWD of the container</span>
<span class="hljs-keyword">COPY</span><span class="bash"> . .</span>

<span class="hljs-comment"># Make the Docker container executable</span>
<span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-string">"python"</span>]</span>

<span class="hljs-comment"># Specify the default command line argument for the entrypoint</span>
<span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">"app.py"</span>]</span>

<span class="hljs-comment"># This will be based to the default entry point</span>
<span class="hljs-comment"># CMD [ "python", "./app.py" ]</span>
</div></div></code></pre>
<ul>
<li>Build the image with tag <code>flask_webservice</code>.</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">build</span> <span class="hljs-bullet">-t</span> <span class="hljs-string">flask_webservice</span> <span class="hljs-string">.</span>
</code></pre>
<ul>
<li>Run the container on port 5000. Add a name to the container <code>flask_webservice</code>.</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">run</span> <span class="hljs-bullet">-d</span> <span class="hljs-bullet">-p</span> <span class="hljs-number">5000</span><span class="hljs-string">:5000</span> <span class="hljs-bullet">--name</span> <span class="hljs-string">test_webservice</span> <span class="hljs-string">flask_webservice</span> 
</code></pre>
<p>The option <code>-p</code> is used to map ports between the host and the container. The option <code>-d</code> is used to detach the shell from the container such that it will run in the background and will not block the shell.</p>
<ul>
<li>Some other commands.</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-comment"># Stops the container</span>
<span class="hljs-string">docker</span> <span class="hljs-string">stop</span> <span class="hljs-string">&lt;container-id-name&gt;</span>

<span class="hljs-comment"># Stops the container</span>
<span class="hljs-string">docker</span> <span class="hljs-string">rm</span> <span class="hljs-string">&lt;container-id-name&gt;</span>

<span class="hljs-comment"># shows all running containers</span>
<span class="hljs-string">docker</span> <span class="hljs-string">ps</span>
</code></pre>
<ul>
<li>Add a volume if you are debugging the application.</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">run</span> <span class="hljs-bullet">-p</span> <span class="hljs-number">5000</span><span class="hljs-string">:5000</span> <span class="hljs-bullet">--name</span> <span class="hljs-string">test_service</span> <span class="hljs-bullet">-v</span> <span class="hljs-string">.:/usr/src/app</span> <span class="hljs-bullet">--rm</span> <span class="hljs-string">flask_webservice</span>
</code></pre>
<p>A Docker volume is an independent file system entirely managed by Docker and exists as a normal file or directory on the host, where data is persisted.</p>
<ul>
<li>Create a repository on Docker hub<br>
<img src="https://i.imgur.com/DQNJGPd.png" alt="" class="md-image md-image"></li>
<li>Log in on your local machine</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">login</span>
</code></pre>
<p>Enter the username and password.</p>
<ul>
<li>Rename the docker image</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">tag</span> <span class="hljs-string">flask_webservice</span> <span class="hljs-string">firasj/dummy_flask_service:v1.0</span>
</code></pre>
<ul>
<li>Push to Docker Hub</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">push</span> <span class="hljs-string">firasj/dummy_flask_service:v1.0</span>
</code></pre>
<p>Whenever you made changes, you can build then push another version/tag to Docker hub.</p>
<ul>
<li>Access the container via terminal</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">docker</span> <span class="hljs-string">exec</span> <span class="hljs-bullet">-i</span> <span class="hljs-bullet">-t</span> <span class="hljs-string">test_service</span> <span class="hljs-string">bash</span>
</code></pre>
<p>If you pass to the previous command <code>app.py</code> instead of <code>bash</code>, then the container will run the python as follows:</p>
<pre><code class="yaml hljs"><span class="hljs-string">python</span> <span class="hljs-string">app.py</span>
</code></pre>
<p>If we pass a different application <code>app2.py</code>, the container will run the python as follows:</p>
<pre><code class="yaml hljs"><span class="hljs-string">python</span> <span class="hljs-string">app2.py</span>
</code></pre>
<p>If the <code>docker</code> command asks for <code>sudo</code> permission everytime you use it, run the following:</p>
<pre><code class="yaml hljs"><span class="hljs-string">sudo</span> <span class="hljs-string">chmod</span> <span class="hljs-number">777</span> <span class="hljs-string">/var/run/docker.sock</span>
</code></pre>
<p>This will give permissions to everyone to run docker (could be risky for multi-user environments).</p>
<h2 id="Docker-Compose"><a class="anchor hidden-xs" href="#Docker-Compose" title="Docker-Compose"><i class="fa fa-link"></i></a>Docker Compose</h2>
<p>Docker Compose is a tool for defining and running multi-container applications.</p>
<div class="alert alert-info">
<p>You can find below some tutorials to learn Docker and Docker compose.</p>
<blockquote>
<p><a href="https://learnxinyminutes.com/docs/docker/" target="_blank" rel="noopener">https://learnxinyminutes.com/docs/docker/</a><br>
A tutorial to learn Docker<br>
<a href="https://docker-curriculum.com/#docker-compose" target="_blank" rel="noopener">https://docker-curriculum.com/#docker-compose</a><br>
A tutorial to learn Docker compose</p>
</blockquote>
</div>
</details><p>You can run Apache Airflow using docker and docker compose. Example on docker and docker compose is below this section.</p><h1 id="Apache-Airflow"><a class="anchor hidden-xs" href="#Apache-Airflow" title="Apache-Airflow"><i class="fa fa-link"></i></a>Apache Airflow</h1><p>Apache Airflow™ is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows. The main characteristic of Airflow workflows is that all workflows are defined in Python code, such that <strong>“Workflows as code”</strong>. It started in 2014 at Airbnb, and is written in Python.</p><p><img src="https://i.imgur.com/oSrp7pc.png" alt="" class="md-image md-image"></p><p>Airflow is a platform that lets you build and run <strong>workflows</strong>. A workflow is represented as a <strong>DAG</strong> (a Directed Acyclic Graph), and contains individual pieces of work called <strong>Tasks</strong>, arranged with dependencies and “data” flows taken into account. Airflow™ is a <strong>batch workflow orchestration platform</strong>.</p><h2 id="Install-Airflow"><a class="anchor hidden-xs" href="#Install-Airflow" title="Install-Airflow"><i class="fa fa-link"></i></a>Install Airflow</h2><p>There are multiple ways to install Aapch Airflow, but here we will demonstrate only <code>pip</code> and <code>Docker</code> mathods.</p><h3 id="1-Usingpip"><a class="anchor hidden-xs" href="#1-Usingpip" title="1-Usingpip"><i class="fa fa-link"></i></a>1. Using<code>pip</code></h3><p>Apache Airflow has no support for Windows platform and we need to find a way to run it using Docker or WSL2. For Linux users. the installation is easier.</p><h4 id="Windows-user"><a class="anchor hidden-xs" href="#Windows-user" title="Windows-user"><i class="fa fa-link"></i></a>Windows user</h4><p>We will use WSL2 system to run Airflow. You need to install WSL2 with the Ubuntu distro.</p><div class="alert alert-danger">
<p>This step is important for Windows users as some tools do not support Windows platform such as <code>feast</code> and <code>airflow</code>.</p>
</div><p>You need to follow the steps below:</p><ol>
<li>Follow this <a href="https://canonical-ubuntu-wsl.readthedocs-hosted.com/en/latest/guides/install-ubuntu-wsl2/" target="_blank" rel="noopener">tutorial</a> to install WSL2 and Ubuntu on Windows. Open Ubuntu app (terminal) on your Windows and configure the username and password. This user has sudo permission and root user is not needed.</li>
<li>Install git on Ubuntu if not installed. Clone your project repository to some folder (e.g. <code>~/project</code> in my PC).</li>
<li>Open the project local repository in VS Code by executing<pre><code class="yaml hljs"><span class="hljs-string">code</span> <span class="hljs-string">&lt;project-folder-path&gt;</span>

<span class="hljs-comment"># OR</span>
<span class="hljs-comment"># Example</span>
<span class="hljs-string">cd</span> <span class="hljs-string">&lt;project-folder-path&gt;</span>
<span class="hljs-string">code</span> <span class="hljs-string">.</span> <span class="hljs-comment"># if you are already in the project local repository</span>
</code></pre>
This will install the extension for the first time and open the project repo in VS code and you can work there. The commands in the terminal of VS code will be executed on Ubuntu via WSL2.</li>
</ol><p>After that, follow the section <a href="#Prepare-the-workspace-tested-on-Ubuntu-2204">Prepare the workspace</a>.</p><h4 id="Linux-user"><a class="anchor hidden-xs" href="#Linux-user" title="Linux-user"><i class="fa fa-link"></i></a>Linux user</h4><p>For Linux users, nothing special here, they just need to go to set up their workspace from <a href="#Prepare-the-workspace-tested-on-Ubuntu-2204">here</a>.</p><h3 id="2-Using-Docker"><a class="anchor hidden-xs" href="#2-Using-Docker" title="2-Using-Docker"><i class="fa fa-link"></i></a>2. Using <code>Docker</code></h3><div class="alert alert-warning">
<ul>
<li>I recommend to follow the first approach (using pip) as every tool will run on the same system but if you prefer this method (with additional efforts), you can try. You need to manage here two OSes and need to install Python packages on both of them.</li>
<li>Make sure that the version of Docker image for Apache Airflow is the same as the package version in your virtual environment in VS code. I recommend the Airflow version <code>2.7.3</code> due to its compatibility with the dependencies of other tools. If you installed more recent versions, you may face issues (conflicts of dependencies).</li>
</ul>
<!-- - and also version `2.7.3` with less features such as `@task.bash` decorator is not defined here. -->
</div><p>The <code>airflow.Dockerfile</code> contains the instructions to build a configured image of Apache Airflow.</p><pre><code class="yaml hljs"><span class="hljs-comment"># The base image - Use the same version of the package `apache-airflow` that you installed via pip</span>
<span class="hljs-string">FROM</span> <span class="hljs-string">apache/airflow:2.7.3-python3.11</span>
<span class="hljs-comment"># FROM apache/airflow:latest-python3.11</span>
<span class="hljs-comment"># FROM apache/airflow:2.9.2-python3.11</span>
<span class="hljs-comment"># Why python3.11? the explanation is later below</span>

<span class="hljs-comment"># Set CWD inside the container</span>
<span class="hljs-string">WORKDIR</span> <span class="hljs-string">/project</span>
<span class="hljs-comment"># This will be the project folder inside the container</span>

<span class="hljs-comment"># Copy requirements file</span>
<span class="hljs-string">COPY</span> <span class="hljs-string">airflow.requirements.txt</span> <span class="hljs-string">.</span>

<span class="hljs-comment"># Install requirements.txt</span>
<span class="hljs-string">RUN</span> <span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-bullet">-r</span> <span class="hljs-string">airflow.requirements.txt</span> <span class="hljs-bullet">--upgrade</span>

<span class="hljs-comment"># Switch to root user</span>
<span class="hljs-string">USER</span> <span class="hljs-string">root</span>

<span class="hljs-comment"># Install some more CLIs</span>
<span class="hljs-string">RUN</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">update</span> <span class="hljs-string">\</span>
<span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">install</span> <span class="hljs-bullet">-y</span> <span class="hljs-bullet">--no-install-recommends</span> <span class="hljs-string">vim</span> <span class="hljs-string">curl</span> <span class="hljs-string">git</span> <span class="hljs-string">rsync</span> <span class="hljs-string">unzip</span> <span class="hljs-string">\</span>
<span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">autoremove</span> <span class="hljs-bullet">-y</span> <span class="hljs-string">\</span>
<span class="hljs-string">&amp;&amp;</span> <span class="hljs-string">apt-get</span> <span class="hljs-string">clean</span>

<span class="hljs-comment"># EXPOSE 8080</span>

<span class="hljs-comment"># Switch to regular user airflow</span>
<span class="hljs-string">USER</span> <span class="hljs-string">airflow</span>

<span class="hljs-comment"># Run this command when we start the container</span>
<span class="hljs-string">CMD</span> <span class="hljs-string">["airflow",</span> <span class="hljs-string">"standalone"</span><span class="hljs-string">]</span>
</code></pre><p>The following file <code>airflow.docker-compose.yaml</code> contains all instructions needed to spin up a docker container to run Airflow components.</p><pre><code class="pug hljs"><span class="hljs-comment"># airflow.docker-compose.yaml</span>
<span class="hljs-comment">
# declare a dict of services</span>
<span class="hljs-attr">services</span>:<span class="hljs-string"></span>
<span class="hljs-comment">
  # Airflow service</span>
  <span class="hljs-attr">airflow</span>:<span class="hljs-string"></span>
<span class="hljs-comment">
    # Container name of thes service</span>
    <span class="hljs-attr">container_name</span>: <span class="hljs-string">"airflow_service"</span>
<span class="hljs-comment">
    # base image</span>
<span class="hljs-comment">    # image: apache/airflow:latest</span>
<span class="hljs-comment">    # Build the custom image</span>
    <span class="hljs-attr">build</span>:<span class="hljs-string"></span>
      <span class="hljs-attr">context</span>: <span class="hljs-string">.</span>
      <span class="hljs-attr">dockerfile</span>: <span class="hljs-string">airflow.Dockerfile</span>
<span class="hljs-comment">
    # Always restart the container if it stops. </span>
<span class="hljs-comment">    # If it's manually stopped, it's restarted </span>
<span class="hljs-comment">    # only when Docker daemon restarts or the container itself is manually restarted.</span>
    <span class="hljs-attr">restart</span>: <span class="hljs-string">always</span>
<span class="hljs-comment">
    # Env variables to be used inside the container</span>
    <span class="hljs-attr">environment</span>:<span class="hljs-string"></span>
<span class="hljs-comment">
      # The home directory of Airflow inside the container</span>
      <span class="hljs-attr">AIRFLOW_HOME</span> : <span class="hljs-string">/project/services/airflow</span>
<span class="hljs-comment">	  
      # The directories of source code and scripts in the project</span>
<span class="hljs-comment">      # These locations will be added to let airflow see our files</span>
      <span class="hljs-attr">PYTHONPATH</span> : <span class="hljs-string">/project/src:/project/scripts</span>
<span class="hljs-comment">
      # A custom environment variable to our project directory</span>
      <span class="hljs-attr">PROJECTPATH</span>: <span class="hljs-string">/project</span>
<span class="hljs-comment">    
    # Allows to create volumes</span>
    <span class="hljs-attr">volumes</span>:<span class="hljs-string"></span>
<span class="hljs-comment">
      # Creates a volume for airflow metadata</span>
      <span class="hljs-meta">-</span> <span class="hljs-string">./services/airflow:/opt/airflow</span>
<span class="hljs-comment">
      # Creates a volume to store the project in the container</span>
      <span class="hljs-meta">-</span> <span class="hljs-string">.:/project</span>
	  
    <span class="hljs-attr">ports</span>:<span class="hljs-string"></span>
<span class="hljs-comment">      # Used to map ports from host to guest (container)</span>
      <span class="hljs-meta">-</span> <span class="hljs-string">8080:8080</span>
<span class="hljs-comment">
    # Command to run when we start the container</span>
<span class="hljs-comment">    # We do not need this since we have it in airflow.Dockerfile</span>
<span class="hljs-comment">    # command: airflow standalone</span>
</code></pre><p>Run the container</p><pre><code class="yaml hljs"><span class="hljs-string">sudo</span> <span class="hljs-string">docker-compose</span> <span class="hljs-bullet">-f</span> <span class="hljs-string">airflow.docker-compose.yaml</span> <span class="hljs-string">up</span> <span class="hljs-bullet">-d</span> <span class="hljs-bullet">--build</span>
</code></pre><p>You can access the web server in (<a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a>). If the port 8080 is not free, then assign it to another port, for instance (8081:8080).</p><p>The default username is <code>admin</code> and the password is randomly generated and stored in <code>./services/airflow/standalone_admin_password.txt</code>.</p><div class="alert alert-warning">
<p><strong>Linux gurus</strong><br>
If you used the Docker approach with Linux systems, you may have some issues with access permission to files. Make sure that the UID of the user running the docker commands the same UID inside the docker container. Check this (<a href="https://github.com/puckel/docker-airflow/issues/224" target="_blank" rel="noopener">https://github.com/puckel/docker-airflow/issues/224</a>).</p>
<p>In linux systems, you have to create some folders in advance for the volumes you mounted. Check this (<a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#initializing-environment" target="_blank" rel="noopener">https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#initializing-environment</a>)</p>
</div><div class="alert alert-info">
<p><strong>Info:</strong></p>
<ul>
<li>If you have good hardware resources, then you can run the production-ready docker compose file from <a href="https://airflow.apache.org/docs/apache-airflow/2.9.2/docker-compose.yaml" target="_blank" rel="noopener">https://airflow.apache.org/docs/apache-airflow/2.9.2/docker-compose.yaml</a>.</li>
<li>This will create a multi-container cluster to run a docker container for each of the Airflow components.</li>
</ul>
</div><h2 id="Prepare-the-workspace-tested-on-Ubuntu-2204"><a class="anchor hidden-xs" href="#Prepare-the-workspace-tested-on-Ubuntu-2204" title="Prepare-the-workspace-tested-on-Ubuntu-2204"><i class="fa fa-link"></i></a>Prepare the workspace [tested on Ubuntu 22.04]</h2><p>Whether you followed <code>pip</code> or <code>Docker</code> method, you need to set up your workspace to run all required tools properly. The steps are:</p><h3 id="1-Install-python311"><a class="anchor hidden-xs" href="#1-Install-python311" title="1-Install-python311"><i class="fa fa-link"></i></a>1. Install <code>python3.11</code></h3><p>Install <code>python3.11</code> and <code>python3.11-venv</code> on your system (Ubuntu in my case) before running any Airflow component.</p><pre><code class="yaml hljs"><span class="hljs-string">sudo</span> <span class="hljs-string">apt</span> <span class="hljs-string">update</span>

<span class="hljs-comment"># install Python 3.11</span>
<span class="hljs-string">sudo</span> <span class="hljs-string">apt</span> <span class="hljs-string">install</span> <span class="hljs-string">python3.11</span>

<span class="hljs-comment"># instal Python 3.11 for creating virtual environments</span>
<span class="hljs-string">sudo</span> <span class="hljs-string">apt</span> <span class="hljs-string">install</span> <span class="hljs-string">python3.11-venv</span>
</code></pre><div class="alert alert-warning">
<p>We are using Python 3.11 here and not 3.12 or 3.13 since some tools such as <code>zenml</code> do not support Python versions higher than 3.11.</p>
</div><h3 id="2-Create-a-new-virtual-environment-using-Python-311"><a class="anchor hidden-xs" href="#2-Create-a-new-virtual-environment-using-Python-311" title="2-Create-a-new-virtual-environment-using-Python-311"><i class="fa fa-link"></i></a>2. Create a new virtual environment using Python 3.11</h3><pre><code class="yaml hljs"><span class="hljs-comment"># Create .venv</span>
<span class="hljs-string">python3.11</span> <span class="hljs-bullet">-m</span> <span class="hljs-string">venv</span> <span class="hljs-string">.venv</span>

<span class="hljs-comment"># Activate it</span>
<span class="hljs-string">source</span> <span class="hljs-string">.venv/bin/activate</span>

<span class="hljs-comment"># You can deactivate it at anytime</span>
<span class="hljs-comment"># deactivate</span>
</code></pre><h3 id="3-Create-requirementstxt-and-install-the-dependencies"><a class="anchor hidden-xs" href="#3-Create-requirementstxt-and-install-the-dependencies" title="3-Create-requirementstxt-and-install-the-dependencies"><i class="fa fa-link"></i></a>3. Create <code>requirements.txt</code> and install the dependencies.</h3><pre><code class="yaml hljs"><span class="hljs-comment"># requirements.txt</span>

<span class="hljs-comment"># HERE I added all possible tools that you will probably  use in the project</span>
<span class="hljs-string">zenml[server]</span>
<span class="hljs-string">apache-airflow</span>
<span class="hljs-string">feast</span>
<span class="hljs-string">great_expectations</span>
<span class="hljs-string">dvc</span>
<span class="hljs-string">hydra-core</span>
<span class="hljs-string">pytest</span>
<span class="hljs-string">pandas</span>
<span class="hljs-string">scikit-learn</span>
<span class="hljs-string">pendulum</span>
<span class="hljs-string">shyaml</span>
<span class="hljs-string">mlflow</span>
<span class="hljs-string">giskard</span>
<span class="hljs-string">evidently</span>
<span class="hljs-string">Flask</span>
<span class="hljs-string">flask-session2</span>
<span class="hljs-string">psycopg2-binary</span>
<span class="hljs-comment"># tensorflow # If you are Master's student</span>
<span class="hljs-comment"># torch # If you are Master's student</span>
</code></pre><div class="alert alert-info">
<pre><code class="yaml hljs"><span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-bullet">-r</span> <span class="hljs-string">requirements.txt</span> <span class="hljs-bullet">--upgrade</span>
</code></pre>
<ul>
<li>Do not use <code>--upgrade</code> if you do not want to change in the installed packages.</li>
<li>Omitting this option, may introduce some conflicts with other packages (be careful).</li>
</ul>
</div><div class="alert alert-warning">
<p>When you want to install a new package, add it to <code>requirements.txt</code> and run the previous commnd again. Do not install packages, one by one as you may face dependency conflicts. In summary, you need to keep all your packages without conflicts.</p>
</div><div class="alert alert-danger">
<p><strong>Important notes:</strong></p>
<ul>
<li>Before running Airflow. We need to customize the Airflow config folder as follows.<pre><code class="bash hljs"><span class="hljs-comment"># PWD is project folder path</span>
<span class="hljs-built_in">export</span> AIRFLOW_HOME=<span class="hljs-variable">$PWD</span>/services/airflow
</code></pre>
</li>
<li>Make sure that you set this environment variable before you work with Airflow. This will store the configs and metadata in the specified folder. This folder needs to be pushed to Github.</li>
<li>Add this to your <code>~/.bashrc</code> file as a permanent variable but replace <code>$PWD</code> with your absolute project folder path.<pre><code class="bash hljs"><span class="hljs-comment"># REPLACE &lt;project-folder-path&gt; with your project folder path</span>
<span class="hljs-built_in">cd</span> &lt;project-folder-path&gt;
<span class="hljs-built_in">echo</span> <span class="hljs-string">"export AIRFLOW_HOME=<span class="hljs-variable">$PWD</span>/services/airflow"</span> &gt;&gt; ~/.bashrc

<span class="hljs-comment"># Run/Load the file content</span>
<span class="hljs-built_in">source</span> ~/.bashrc

<span class="hljs-comment"># Activate the virtual environment again</span>
<span class="hljs-built_in">source</span> .venv/bin/activate
</code></pre>
<ul>
<li>You do this only once, then the variable will be loaded everytime you open a new terminal.</li>
</ul>
</li>
</ul>
</div><h3 id="4-Setup-the-Airflow-components"><a class="anchor hidden-xs" href="#4-Setup-the-Airflow-components" title="4-Setup-the-Airflow-components"><i class="fa fa-link"></i></a>4. Setup the Airflow components</h3><h4 id="41-Setup-Metadata-database-and-Executor"><a class="anchor hidden-xs" href="#41-Setup-Metadata-database-and-Executor" title="41-Setup-Metadata-database-and-Executor"><i class="fa fa-link"></i></a>4.1. Setup Metadata database and Executor</h4><h5 id="41A-Minimal-setup-SequentialExecutor--SQLite"><a class="anchor hidden-xs" href="#41A-Minimal-setup-SequentialExecutor--SQLite" title="41A-Minimal-setup-SequentialExecutor--SQLite"><i class="fa fa-link"></i></a>4.1.A. Minimal setup (<code>SequentialExecutor</code> + SQLite)</h5><ul>
<li>Initialize the metadata database (SQLite)</li>
</ul><pre><code class="bash hljs"><span class="hljs-comment"># Clean it </span>
<span class="hljs-comment"># Caution: this will delete everything</span>
airflow db reset

<span class="hljs-comment"># initialize the metadata database</span>
airflow db init
</code></pre><div class="alert alert-danger">
<p><strong>Note:</strong><br>
This minimal setup of Airflow will use <code>SequentialExecutor</code> which runs only a single thread, so you cannot run more than one task at the same time. The database is a lightweight <code>sqlite</code> database. The drawback here is mainly related to the performance and resilience of Airflow components. You may face issues such as connection timeout for database, the scheduler is unhealthy (need to restart it), more waiting time for DAG runs since only one thread is allowed here.</p>
</div><h5 id="41B-Normal-setup-LocalExecutor--PostgreSQL"><a class="anchor hidden-xs" href="#41B-Normal-setup-LocalExecutor--PostgreSQL" title="41B-Normal-setup-LocalExecutor--PostgreSQL"><i class="fa fa-link"></i></a>4.1.B. Normal setup (<code>LocalExecutor</code> + PostgreSQL)</h5><ul>
<li>Install PostgreSQL on your PC.</li>
</ul><pre><code class="bash hljs">sudo apt-get install postgresql
</code></pre><p>The output:</p><pre><code class="bash hljs">firasj@Lenovo:~$ sudo apt-get install postgresql
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
Suggested packages:
  postgresql-doc
The following NEW packages will be installed:
  postgresql
0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.
Need to get 0 B/3288 B of archives.
After this operation, 71.7 kB of additional disk space will be used.
Selecting previously unselected package postgresql.
(Reading database ... 37631 files and directories currently installed.)
Preparing to unpack .../postgresql_14+238_all.deb ...
Unpacking postgresql (14+238) ...
Setting up postgresql (14+238) ...
</code></pre><ul>
<li>Run PostgreSQL service</li>
</ul><pre><code class="bash hljs">sudo systemctl start postgresql
</code></pre><p>The output:</p><pre><code class="bash hljs">firasj@Lenovo:~$ sudo systemctl start postgresql
</code></pre><ul>
<li>Create a database user for you
<ul>
<li>Access the <code>psql</code> CLI as default <code>postgres</code> user</li>
</ul>
<pre><code class="bash hljs">firasj@Lenovo:~$ sudo -u postgres psql
psql (14.12 (Ubuntu 14.12-0ubuntu0.22.04.1))
Type <span class="hljs-string">"help"</span> <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>.

postgres=<span class="hljs-comment">#</span>
</code></pre>
<ul>
<li>Create a database user for you (<code>firasj</code> in my PC)</li>
</ul>
<pre><code class="pgsql hljs">postgres=# <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">USER</span> firasj <span class="hljs-keyword">WITH</span> <span class="hljs-keyword">PASSWORD</span> <span class="hljs-string">'firasj'</span>;

</code></pre>
</li>
<li>Create a database <code>aiflow</code></li>
</ul><pre><code class="pgsql hljs">postgres=# <span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">DATABASE</span> airflow;
</code></pre><ul>
<li>Give permissions to the database user  (<code>firasj</code> in my PC).</li>
</ul><pre><code class="pgsql hljs">postgres=# <span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">PRIVILEGES</span> <span class="hljs-keyword">ON</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">TABLES</span> <span class="hljs-keyword">IN</span> <span class="hljs-keyword">SCHEMA</span> <span class="hljs-built_in">public</span> <span class="hljs-keyword">TO</span> firasj;
</code></pre><ul>
<li>Check that the database user is created.</li>
</ul><pre><code class="pgsql hljs">postgres=# \du

                                   List <span class="hljs-keyword">of</span> roles
 <span class="hljs-keyword">Role</span> <span class="hljs-type">name</span> |                         Attributes                         | Member <span class="hljs-keyword">of</span>
<span class="hljs-comment">-----------+------------------------------------------------------------+-----------</span>
 firasj    |                                                            | {}
 postgres  | <span class="hljs-keyword">Superuser</span>, <span class="hljs-keyword">Create</span> <span class="hljs-keyword">role</span>, <span class="hljs-keyword">Create</span> DB, <span class="hljs-keyword">Replication</span>, Bypass RLS | {}
</code></pre><ul>
<li>Close the connection</li>
</ul><pre><code class="pgsql hljs">postgres=# \q
</code></pre><ul>
<li>Configure Postgresql permissions</li>
</ul><pre><code class="bash hljs"><span class="hljs-comment"># 1. Open this file</span>
sudo nano /etc/postgresql/14/main/pg_hba.conf

<span class="hljs-comment"># 2. Add the following line to the end of this file</span>
host all all 0.0.0.0/0 trust

<span class="hljs-comment"># 3. Save the change and close it</span>

<span class="hljs-comment"># 4. Open another file</span>
sudo nano /etc/postgresql/14/main/postgresql.conf

<span class="hljs-comment"># 5. Add the line as follows</span>

<span class="hljs-comment">#------------------------------------------------------------------------------</span>
<span class="hljs-comment"># CONNECTIONS AND AUTHENTICATION</span>
<span class="hljs-comment">#------------------------------------------------------------------------------</span>

<span class="hljs-comment"># - Connection Settings -</span>

listen_addresses = <span class="hljs-string">'*'</span>

<span class="hljs-comment"># 6. Save the change and close it</span>
</code></pre><ul>
<li>Restart PostgreSQL service</li>
</ul><pre><code class="bash hljs">firasj@Lenovo:~$ sudo systecmtl restart postgresql
</code></pre><ul>
<li>Configure the Airflow.</li>
</ul><pre><code class="bash hljs">
<span class="hljs-comment"># 1. Go to the file `./services/airflow/airflow.cfg` in VS Code</span>

<span class="hljs-comment"># 2. Change the executor</span>
executor = LocalExecutor  <span class="hljs-comment"># line 45</span>

<span class="hljs-comment"># 3. Change the database connection </span>
<span class="hljs-comment"># line 424</span>
sql_alchemy_conn = postgresql+psycopg2://firasj:firasj@localhost:5432/airflow
<span class="hljs-comment"># This connection string has the following format</span>
<span class="hljs-comment"># postgresql+psycopg2://&lt;db_user&gt;:&lt;db_user_password&gt;@&lt;hostname&gt;:&lt;port_number&gt;/&lt;database_name&gt;</span>
</code></pre><ul>
<li>Initialize the metadata database (PostgreSQL)</li>
</ul><pre><code class="bash hljs"><span class="hljs-comment"># Clean it </span>
<span class="hljs-comment"># Caution: this will delete everything</span>
airflow db reset

<span class="hljs-comment"># initialize the metadata database</span>
airflow db init
</code></pre><p>Now you are ready to setup your Airflow components and run them.</p><h4 id="42-Setup-Airflow-webserver"><a class="anchor hidden-xs" href="#42-Setup-Airflow-webserver" title="42-Setup-Airflow-webserver"><i class="fa fa-link"></i></a>4.2. Setup Airflow webserver</h4><ul>
<li>Create a new user and add it to the database.</li>
</ul><pre><code class="wrap yaml hljs"><span class="hljs-comment"># Here we are creating admin user with Admin role</span>
<span class="hljs-string">airflow</span> <span class="hljs-string">users</span> <span class="hljs-string">create</span> <span class="hljs-bullet">--role</span> <span class="hljs-string">Admin</span> <span class="hljs-bullet">--username</span> <span class="hljs-string">admin</span> <span class="hljs-bullet">--email</span> <span class="hljs-string">admin@example.org</span> <span class="hljs-bullet">--firstname</span> <span class="hljs-string">admin</span> <span class="hljs-bullet">--lastname</span> <span class="hljs-string">admin</span> <span class="hljs-bullet">--password</span> <span class="hljs-string">admin</span>
</code></pre><blockquote>
<p>You can check your user from the list as follows:</p>
<pre><code class="yaml hljs"><span class="hljs-string">airflow</span> <span class="hljs-string">users</span> <span class="hljs-string">list</span>
</code></pre>
</blockquote><div class="alert alert-warning">
<p>The webserver will usually load examples of workflows, you can disable that from <code>./services/airflow/airflow.cfg</code>.</p>
<pre><code class="yaml hljs"><span class="hljs-comment"># line 124</span>
<span class="hljs-string">load_examples</span> <span class="hljs-string">=</span> <span class="hljs-literal">False</span>
</code></pre>
<p>Restart the webserver to see the change if it is already running.</p>
</div><div class="alert alert-warning">
<p>The default port for the Airflow webserver is 8080. You can change from the configuration file <code>./services/airflow/airflow.cfg</code>.</p>
<pre><code class="yaml hljs"><span class="hljs-comment"># line 1320</span>
<span class="hljs-string">web_server_port</span> <span class="hljs-string">=</span> <span class="hljs-number">8080</span>
</code></pre>
<p>Restart the webserver to see the change if it is already running.</p>
</div><h4 id="43-Run-Airflow-components"><a class="anchor hidden-xs" href="#43-Run-Airflow-components" title="43-Run-Airflow-components"><i class="fa fa-link"></i></a>4.3. Run Airflow components</h4><ul>
<li>Setup the environment and create some directories in airflow home directory.</li>
</ul><pre><code class="console hljs">
<span class="hljs-comment"># Add the folders which contain source codes where you want Airflow to import them for pipelines</span>
<span class="hljs-built_in">export</span> PYTHONPATH=<span class="hljs-variable">$PWD</span>/src
<span class="hljs-comment"># You can do this also from the code via appending to sys.path</span>

<span class="hljs-comment"># REPLACE &lt;project-folder-path&gt; with your project folder path</span>
<span class="hljs-built_in">cd</span> &lt;project-folder-path&gt;
<span class="hljs-built_in">echo</span> <span class="hljs-string">"export PYTHONPATH=<span class="hljs-variable">$PWD</span>/src"</span> &gt;&gt; ~/.bashrc

<span class="hljs-comment"># Run/Load the file content</span>
<span class="hljs-built_in">source</span> ~/.bashrc

<span class="hljs-comment"># Activate the virtual environment again</span>
<span class="hljs-built_in">source</span> .venv/bin/activate

<span class="hljs-comment"># Create folders and files for logging the output of components</span>
mkdir -p <span class="hljs-variable">$AIRFLOW_HOME</span>/logs <span class="hljs-variable">$AIRFLOW_HOME</span>/dags
<span class="hljs-built_in">echo</span> &gt; <span class="hljs-variable">$AIRFLOW_HOME</span>/logs/scheduler.log
<span class="hljs-built_in">echo</span> &gt; <span class="hljs-variable">$AIRFLOW_HOME</span>/logs/triggerer.log
<span class="hljs-built_in">echo</span> &gt; <span class="hljs-variable">$AIRFLOW_HOME</span>/logs/webserver.log

<span class="hljs-comment"># Add log files to .gitignore</span>
<span class="hljs-built_in">echo</span> *.<span class="hljs-built_in">log</span> &gt;&gt; <span class="hljs-variable">$AIRFLOW_HOME</span>/logs/.gitignore
</code></pre><ul>
<li>Start the scheduler component</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">airflow</span> <span class="hljs-string">scheduler</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-bullet">--log-file</span> <span class="hljs-string">services/airflow/logs/scheduler.log</span>
</code></pre><ul>
<li>Start the webserver component</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">airflow</span> <span class="hljs-string">webserver</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-bullet">--log-file</span> <span class="hljs-string">services/airflow/logs/webserver.log</span>
</code></pre><ul>
<li>[optional] Start the triggerer component
<ul>
<li>used to run deferrable operators</li>
</ul>
</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">airflow</span> <span class="hljs-string">triggerer</span> <span class="hljs-bullet">--daemon</span> <span class="hljs-bullet">--log-file</span> <span class="hljs-string">services/airflow/logs/triggerer.log</span>
</code></pre><ul>
<li>You can omit <code>--daemon</code> if do not want to daemonize it.</li>
<li>You can omit <code>--log-file</code> if you are not interested in the log.</li>
</ul><div class="alert alert-warning">
<p>If you want to kill all Airflow processes/daemons in the background, run as follows:</p>
<pre><code class="yaml hljs"><span class="hljs-string">kill</span> <span class="hljs-string">$(ps</span> <span class="hljs-bullet">-ef</span> <span class="hljs-string">| grep "airflow" | awk '{print $2}')
</span></code></pre>
</div><p>You can access the webserver UI in (<a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a>). The password is <code>admin</code> and username is <code>admin</code> if you did not change it when you created the <code>admin</code> user.</p><div class="alert alert-danger">
<p><strong>Important note: Restart Airflow compnents</strong><br>
If you terminated the Airflow components and want to run them again. Before you start the components again, you should do as follows:</p>
<ol>
<li>Access the project folder and activate the virtual environment.</li>
<li>Set the AIRFLOW_HOME directory to the folder <code>./services/airflow</code> (Where PWD is the project folder) <strong>only if you did not add it to <code>~/.bashrc</code></strong>.<pre><code class="yaml hljs"><span class="hljs-string">export</span> <span class="hljs-string">AIRFLOW_HOME=$PWD/services/airflow</span>
</code></pre>
</li>
<li>Run the components again.</li>
</ol>
<p><strong>Note:</strong> If you do not do these steps then Airflow will not use the metadata and configs you defined in your airflow path (<code>services/airflow</code>).<br>
<strong>Note:</strong> If you see such errors in the log of the components, such as in the scheduler</p>
<pre><code class="ruby hljs">File <span class="hljs-symbol">exists:</span> <span class="hljs-string">'/home/firasj/project/services/airflow/airflow-scheduler.pid'</span>
</code></pre>
<p>Then delete the pid file of the component.</p>
</div><div class="alert alert-warning">
<p><strong>Notes:</strong></p>
<ul>
<li>
<p>Before you start writing DAG definition files, it is good idea to create a soft link <code>pipelines</code> to the folder <code>services/airflow/dags</code> such that you work in <code>pipelines</code> folder and the work will be mirrored in the Airflow <code>services/airflow/dags</code> folder to let Airflow server see our pipelines. The folder <code>services/airflow/dags</code> is the original place for storing DAGs/pipelines and it is the location where DAGs are visible to the Airflow webserver.</p>
</li>
<li>
<p>You can create junction/symbolic link between folders where <code>pipelines</code> is the link and <code>services\airflow\dags</code> is the target, as follows:</p>
<div class="alert alert-info">
<blockquote>
<p>Windows</p>
<pre><code class="yaml hljs"><span class="hljs-comment"># You need to run this in cmd with Admin rights</span>
<span class="hljs-string">mklink</span> <span class="hljs-string">/D</span> <span class="hljs-string">/J</span> <span class="hljs-string">pipelines</span> <span class="hljs-string">services\airflow\dags</span>
</code></pre>
</blockquote>
<blockquote>
<p>Linux (<em>tested on Ubuntu 22.04</em>)</p>
<pre><code class="yaml hljs"><span class="hljs-string">ln</span> <span class="hljs-bullet">-s</span> <span class="hljs-string">./services/airflow/dags</span> <span class="hljs-string">./pipelines</span> 
</code></pre>
</blockquote>
</div>
</li>
</ul>
</div><div class="alert alert-danger">
<p>If the DAG is not visible in Airflow UI then, you may see import errors in the top section of the DAGs page in Airflow web UI:<br>
<img src="https://i.imgur.com/LGimB5Y.png" alt="" class="md-image md-image"></p>
<p>Or (access the Airflow container and) run the command <code>airflow dags list-import-errors</code>. You can run the command <code>airflow dags list</code> to list all dags.</p>
</div><div class="alert alert-warning">
<p>If you got such error:</p>
<p><img src="https://i.imgur.com/36P8MTe.png" alt="" class="md-image md-image"></p>
<p>Try to migrate the metadata database to the latest version as follows:</p>
<pre><code class="bash hljs">airflow db migrate
</code></pre>
<p>Then try to access the home page of the webserver again.</p>
</div><h2 id="Hello-world-workflow"><a class="anchor hidden-xs" href="#Hello-world-workflow" title="Hello-world-workflow"><i class="fa fa-link"></i></a><em>Hello world</em> workflow</h2><p><strong>Note:</strong> Read the notes above before you build your first DAG.</p><pre><code class="pug hljs"><div class="wrapper"><div class="gutter linenumber"><span data-linenumber="1"></span>
<span data-linenumber="2"></span>
<span data-linenumber="3"></span>
<span data-linenumber="4"></span>
<span data-linenumber="5"></span>
<span data-linenumber="6"></span>
<span data-linenumber="7"></span>
<span data-linenumber="8"></span>
<span data-linenumber="9"></span>
<span data-linenumber="10"></span>
<span data-linenumber="11"></span>
<span data-linenumber="12"></span>
<span data-linenumber="13"></span>
<span data-linenumber="14"></span>
<span data-linenumber="15"></span>
<span data-linenumber="16"></span>
<span data-linenumber="17"></span>
<span data-linenumber="18"></span>
<span data-linenumber="19"></span>
<span data-linenumber="20"></span>
<span data-linenumber="21"></span>
<span data-linenumber="22"></span>
<span data-linenumber="23"></span>
<span data-linenumber="24"></span>
<span data-linenumber="25"></span>
<span data-linenumber="26"></span>
<span data-linenumber="27"></span>
<span data-linenumber="28"></span></div><div class="code"><span class="hljs-comment"># pipelines/hello_dag.py</span>

<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime

<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> task
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator

<span class="hljs-comment"># A DAG represents a workflow, a collection of tasks</span>
<span class="hljs-comment"># This DAG is scheduled to print 'hello world' every minute starting from 01.01.2022.</span>
<span class="hljs-keyword">with</span> DAG(dag_id=<span class="hljs-string">"hello_world"</span>, 
		 start_date=datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
		 schedule=<span class="hljs-string">"* * * * *"</span>) <span class="hljs-keyword">as</span> dag:
	
    <span class="hljs-comment"># Tasks are represented as operators</span>
    <span class="hljs-comment"># Use Bash operator to create a Bash task</span>
    hello = BashOperator(task_id=<span class="hljs-string">"hello"</span>, bash_command=<span class="hljs-string">"echo hello"</span>)
	
    <span class="hljs-comment"># Python task</span>
<span class="hljs-meta">    @task()</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">world</span><span class="hljs-params">()</span>:</span>
        print(<span class="hljs-string">"world"</span>)
		
    <span class="hljs-comment"># Set dependencies between tasks</span>
    <span class="hljs-comment"># First is hello task then world task</span>
    hello &gt;&gt; world()


</div></div></code></pre><p>In this workflow, we can see two tasks. The first one is defined using operators and the second one is defined usig <code>@task</code> decorator (TaskFlow API).</p><p><img src="https://i.imgur.com/GbLivcO.png" alt="" class="md-image md-image"></p><h2 id="Core-concepts"><a class="anchor hidden-xs" href="#Core-concepts" title="Core-concepts"><i class="fa fa-link"></i></a>Core concepts</h2><ul>
<li><strong>DAG</strong>
<ul>
<li>Directed Acyclic Graph. An Airflow DAG is a workflow defined as a graph, where all dependencies between nodes are directed and nodes do not self-reference, meaning there are no circular dependencies.</li>
<li>contains individual pieces of work called <strong>Tasks</strong>, arranged with dependencies and “data” flows taken into account.</li>
<li>allows to exchange data between tasks via XComs. XComs are not built for transferring large data between tasks.</li>
<li>Airflow is built for <strong>orchestrating workflows and not data flows</strong>.</li>
</ul>
</li>
</ul><div class="alert alert-info">
<p><strong>Data orchestration</strong> is an automated process for taking siloed data from multiple storage locations, combining and organizing it, and making it available for analysis.</p>
</div><div class="alert alert-warning">
<p>We will use ZenML as data orchestrator and Airflow as workflow orchestrator.</p>
</div><ul>
<li>
<p><strong>DAG run</strong></p>
<ul>
<li>The execution of a DAG at a specific point in time.</li>
<li>A DAG run can be one of four different types: <code>scheduled</code>, <code>manual</code>, <code>dataset_triggered</code> or <code>backfill</code>.</li>
</ul>
</li>
<li>
<p><strong>Operator</strong></p>
<ul>
<li>A unit of work for Airflow to complete such as BashOperator, PythonOperator,…etc</li>
<li>Each operator includes unique arguments for the type of work it is completing.</li>
</ul>
</li>
<li>
<p><strong>Task</strong></p>
<ul>
<li>A step in a DAG describing a single unit of work.</li>
<li>Determines how to execute your operator’s work within the context of a DAG</li>
<li>To use an operator in a DAG, you have to instantiate it as a task.</li>
</ul>
</li>
<li>
<p><strong>Task instance</strong></p>
<ul>
<li>The execution of a task at a specific point in time.</li>
</ul>
</li>
<li>
<p>Airflow tasks are defined in Python code. You can define tasks using:</p>
<ul>
<li>Decorators(<code>@task</code> and <code>@dag</code>) : The recommended way to create pipelines</li>
<li>Traditional API (Operators and DAG)</li>
</ul>
<div class="alert alert-warning">
<p><strong>Note</strong></p>
<ul>
<li>Keep tasks as atomic as possible, meaning that each task performs a single action.</li>
<li>Tasks should be idempotent, which means they produce the same output every time they are run with the same input.</li>
</ul>
</div>
</li>
<li>
<p>The possible states for a Task Instance are:<br>
<img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/task_lifecycle_diagram.png" alt="" class="md-image md-image"></p>
</li>
<li>
<p>Special types of operators</p>
<ul>
<li><strong>Sensors</strong>:
<ul>
<li>are Operators that keep running until a certain condition is fulfilled.</li>
<li>For example, the <code>ExternalTaskSensor</code> waits for a task in an external DAG to run. Sensors wait until a user defined set of criteria are fufilled.</li>
</ul>
</li>
<li><strong>Deferrable Operators: [advanced topic]</strong>
<ul>
<li>Explanation is later.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>XComs</strong> is short for cross-communication, you can use XCom to pass information between your Airflow tasks.</p>
</li>
</ul><h2 id="Airflow-components"><a class="anchor hidden-xs" href="#Airflow-components" title="Airflow-components"><i class="fa fa-link"></i></a>Airflow components</h2><ul>
<li><strong>Scheduler</strong>
<ul>
<li>handles both triggering scheduled workflows, and submitting Tasks to the executor to run.</li>
<li>The <strong>executor</strong>, is a configuration property of the scheduler, not a separate component and runs within the scheduler process.</li>
</ul>
</li>
<li><strong>Webserver</strong>
<ul>
<li>presents a handy user interface to inspect, trigger and debug the behaviour of DAGs and tasks.</li>
</ul>
</li>
<li><strong>A metadata database</strong>
<ul>
<li>used to store state of workflows and tasks.</li>
</ul>
</li>
<li>A <strong>folder of DAG files</strong> is read by the scheduler to figure out what tasks to run and when and to run them.</li>
<li><strong>Triggerrer</strong> [Optional]
<ul>
<li>executes deferred tasks in an asyncio event loop.</li>
</ul>
</li>
</ul><p><img src="https://www.altexsoft.com/static/blog-post/2023/11/fc3dd59e-998e-4288-aac1-f172a9204832.jpg" alt="" class="md-image md-image"></p><h2 id="DAG-Scheduling"><a class="anchor hidden-xs" href="#DAG-Scheduling" title="DAG-Scheduling"><i class="fa fa-link"></i></a>DAG Scheduling</h2><p>One of the fundamental features of Apache Airflow is the ability to schedule jobs. Historically, Airflow users scheduled their DAGs by specifying a schedule with a cron expression, a timedelta object, or a preset Airflow schedule. Recent versions of Airflow have added new ways to schedule DAGs, such as data-aware scheduling with datasets.</p><h3 id="Scheduling-concepts"><a class="anchor hidden-xs" href="#Scheduling-concepts" title="Scheduling-concepts"><i class="fa fa-link"></i></a>Scheduling concepts</h3><ul>
<li><strong>Data interval</strong>: A property of each DAG run that represents the time range it operates in.
<ul>
<li>For example, for a DAG scheduled hourly, each data interval begins at the top of the hour (minute 0) and ends at the close of the hour (minute 59). The DAG run is typically executed at the end of the data interval.</li>
<li>For a DAG scheduled with @daily, for example, each of its data interval would start each day at midnight (00:00) and end at midnight (24:00).</li>
</ul>
</li>
</ul><div class="alert alert-info">
<p>A DAG run is usually scheduled <strong>after</strong> its associated data interval has ended, to ensure the run is able to collect all the data within the time period. In other words, a run covering the data period of 2020-01-01 generally does not start to run until 2020-01-01 has ended, i.e. after 2020-01-02 00:00:00.</p>
</div><ul>
<li><strong>Logical Date</strong>: The start of the data interval. It does not represent when the DAG will be executed. In other words, a DAG run will only be scheduled one interval after <code>start_date</code> which points to the same logical date.</li>
</ul><ul>
<li><strong>Run After</strong>: The earliest time the DAG can be scheduled. This date is shown in the Airflow UI, and may be the same as the end of the data interval depending on your DAG’s timetable.</li>
<li><strong>Backfilling and Catchup</strong>: are related to scheduling failed or past runs.
<ul>
<li><em>Catchup</em>: The scheduler, by default, will kick off a DAG Run for any data interval that has not been run since the last data interval (or has been cleared).</li>
<li><em>Backfill</em>: There can be the case when you may want to run the DAG for a specified historical period e.g., A data filling DAG is created with start_date 2019-11-21, but another user requires the output data from a month ago i.e., 2019-10-21. This process is known as Backfill. This can be done from command line as follows:<pre><code class="yaml hljs"><span class="hljs-string">airflow</span> <span class="hljs-string">dags</span> <span class="hljs-string">backfill</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">            -</span><span class="hljs-bullet">-start-date</span> <span class="hljs-string">START_DATE</span> <span class="hljs-string">\</span>
<span class="hljs-bullet">            -</span><span class="hljs-bullet">-end-date</span> <span class="hljs-string">END_DATE</span> <span class="hljs-string">\</span>
            <span class="hljs-string">dag_id</span>
</code></pre>
The backfill command will re-run all the instances of the dag_id for all the intervals within the start date and end date</li>
<li><code>start_time</code> is the timestamp from which the scheduler will attempt to backfill</li>
</ul>
</li>
<li><strong>DAGs scheduling parameters</strong>
<ul>
<li><code>data_interval_start</code>: Defines the start date and time of the data interval. This parameter is created automatically by Airflow.</li>
<li><code>data_interval_end</code>: Defines the end date and time of the data interval. This parameter is created automatically by Airflow.</li>
<li><code>schedule</code>: Defines when a DAG will be run. The default schedule is <code>timedelta(days=1)</code>, which runs the DAG once per day if no schedule is defined. If you trigger your DAG externally, set the schedule to <code>None</code>.
<ul>
<li><code>schedule_interval</code>: In Airflow 2.3 and earlier, the <code>schedule_interval</code> is used instead of the <code>schedule</code> parameter and it only accepts cron expressions or timedelta objects.</li>
</ul>
</li>
<li><code>start_date</code>: The first date your DAG will be executed. This parameter is required for your DAG to be scheduled by Airflow.</li>
<li><code>end_date</code>: The last date your DAG will be executed. This parameter is optional.</li>
</ul>
</li>
</ul><h4 id="Example"><a class="anchor hidden-xs" href="#Example" title="Example"><i class="fa fa-link"></i></a>Example</h4><p>To demonstrate how these concepts work together, consider a DAG that is scheduled to run <em>every 5 minutes</em> as shown in the image below.</p><p><img src="https://www.astronomer.io/docs/assets/images/2_4_5minExample-2398badb0335832b04874388f49116d4.png" alt="" class="md-image md-image"></p><ul>
<li><code>Logical date</code>: 2022-08-28 22:<strong>37</strong>:33 UTC.
<ul>
<li>Displayed in <code>Data interval start</code> field</li>
</ul>
</li>
<li><code>Data interval end</code>: 2022-08-28 22:<strong>42</strong>:33 UTC.
<ul>
<li>5 minutes later</li>
</ul>
</li>
<li><code>Logical date of Next DAG run</code>: 2022-08-28 22:<strong>42</strong>:33 UTC.
<ul>
<li>Displayed in <code>Next Run</code> field</li>
<li>5 minutes after the previous logical date</li>
<li>Same as <code>Data interval end</code> of previous DAG run<br>
<img src="https://www.astronomer.io/docs/assets/images/2_4_5minExample_next_run-c03246079b133a5c8f84353eaf4cec83.png" alt="" class="md-image md-image"></li>
</ul>
</li>
<li><code>Run After</code>: 2022-08-28 22:<strong>47</strong>:33 UTC.
<ul>
<li>You can see if you hover over <code>Next Run</code>.</li>
<li>The date and time that the next DAG run will actually start</li>
<li>The same as the value in the <code>Data interval</code> end for the next DAG run.</li>
</ul>
</li>
</ul><p>The following is a comparison of the two successive DAG runs:</p><ul>
<li>DAG run 1 (<code>scheduled__2022-08-28T22:37:33.620191+00:00</code>) has a logical date of <code>2022-08-28 22:37:33</code>, a data interval start of <code>2022-08-28 22:37:33</code> and a data interval end of <code>2022-08-28 22:42:33</code>. This DAG run will actually start at <code>2022-08-28 22:42:33</code>.</li>
<li>DAG run 2 (<code>scheduled__2022-08-28T22:42:33.617231+00:00</code>) has a logical date of <code>2022-08-28 22:42:33</code>, a data interval start of <code>2022-08-28 22:42:33</code> and a data interval end of <code>2022-08-28 22:47:33</code>. This DAG run will actually start at <code>2022-08-28 22:47:33</code>.</li>
</ul><div class="alert alert-danger">
<p><strong>Why Airflow starts after the data interval?</strong><br>
Airflow was originally developed for extract, transform, and load (ETL) operations with the expectation that data is constantly flowing in from some source and then will be summarized at a regular interval. However, if you want to summarize data from <em>Monday</em>, you need to wait until <em>Tuesday at 12:01 AM</em>. This shortcoming led to the introduction of <strong>timetables</strong> in Airflow 2.2+. This is an advanced level of scheduling in Airflow and we will not cover it.</p>
</div><p>For pipelines with straightforward scheduling needs, you can define a     <code>schedule</code> (or <code>schedule_interval</code>) in your DAG using:</p><ul>
<li>A cron expression.
<ul>
<li>We set <strong>time interval</strong>.</li>
<li>For example, if you want to schedule your DAG at <code>4:05 AM every day</code>, you would use schedule=‘5 4 * * *’.</li>
</ul>
</li>
<li>A cron preset.
<ul>
<li>For example, schedule=’@hourly’ will schedule the DAG to run at the beginning of every hour.</li>
</ul>
</li>
<li>A <code>timedelta</code> object.
<ul>
<li>If you want to schedule your DAG on a particular rhythm (hourly, every 5 minutes, etc.) rather than at a specific time, you can pass a <code>timedelta</code> object imported from the <code>datetime</code> package to the <code>schedule</code> parameter.</li>
<li>Here we set <strong>time duration</strong> and not time interval.</li>
<li>For example, <code>schedule=timedelta(minutes=30)</code> will run the DAG every thirty minutes, and <code>schedule=timedelta(days=1)</code> will run the DAG every day.</li>
</ul>
</li>
</ul><div class="alert alert-info">
<p>If your DAG does not need to run on a schedule and will only be triggered <code>manually</code> or <code>externally</code> triggered by another process, you can set <code>schedule=None</code>.</p>
</div><div class="alert alert-warning">
<p>DAG should run idempotent (able to be re-run without changing the results), so do <strong>not</strong> use <code>datetime.now()</code> for scheduling.</p>
</div><div class="alert alert-info">
<h4 id="Cron-Expressions-Extra-section"><a class="anchor hidden-xs" href="#Cron-Expressions-Extra-section" title="Cron-Expressions-Extra-section"><i class="fa fa-link"></i></a>Cron Expressions [Extra section]</h4>
<details>
<ul>
<li>
<p><code>cron</code> is a basic utility available on Unix-based systems. It enables users to schedule tasks to run periodically at a specified date/time.</p>
</li>
<li>
<p>Cron runs as a <em>daemon</em> process. This means it only needs to be started once and it will keep running in the background.</p>
</li>
<li>
<p>A cron schedule is a simple text file located under <code>/var/spool/cron/crontabs</code> on Linux systems. We cannot edit the crontab files directly, so we need to access it using the <code>crontab</code> command. To open crontab file, we need to run the following command:</p>
<pre><code class="yaml hljs"><span class="hljs-string">crontab</span> <span class="hljs-bullet">-e</span>
</code></pre>
</li>
<li>
<p>Each line in crontab is an entry with an expression and a command to run:</p>
<pre><code class="yaml hljs"><span class="hljs-string">*</span> <span class="hljs-string">*</span> <span class="hljs-string">*</span> <span class="hljs-string">*</span> <span class="hljs-string">*</span> <span class="hljs-string">echo</span> <span class="hljs-string">hello_cron</span> <span class="hljs-string">&gt;&gt;</span> <span class="hljs-string">~/cron_hello.txt</span>
</code></pre>
</li>
<li>
<p>The cron expression consists of five fields:</p>
<pre><code class="tiddlywiki hljs"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>minute</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>hour</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>day-of-month</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>month</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>day-of-week</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>command</span><span class="token punctuation">&gt;</span></span>
</code></pre>
</li>
</ul>
<p>The range of each field is: &lt;minute&gt; (0-59), &lt;hour&gt; (0-23), &lt;day-of-month&gt; (1-31), &lt;month&gt; (1-12), &lt;day-of-week&gt; (0-6).</p>
<ul>
<li><strong>Special Characters in Expression</strong>
<ul>
<li><code>*</code> (all) specifies that event should happen for every time unit. For example, <code>*</code> in the &lt;minute&gt; field means “for every minute”.</li>
<li><code>?</code> (any) is utilized in the &lt;day-of-month&gt; and &lt;day-of -week&gt; fields to denote the arbitrary value and thus neglect the field value. For example, if we want to run a script at “5th of every month” irrespective of what day of the week falls on that date, we specify a “?” in the &lt;day-of-week&gt; field.</li>
<li><code>-</code> (range) determines the value range. For example, “10-11” in the &lt;hour&gt; field means “10th and 11th hours”.</li>
<li><code>,</code> (values) specifies multiple values. For example, “MON, WED, FRI” in &lt;day-of-week&gt; field means on the days “Monday, Wednesday and Friday”.</li>
<li><code>/</code> (increments) specifies the incremental values. For example, a “5/15” in the &lt;minute&gt; field means at “5, 20, 35 and 50 minutes of an hour”.</li>
</ul>
</li>
</ul>
<!-- 	- `L` (last) has different meanings when used in various fields. For example, if it is applied in the \&lt;day-of-month\&gt; field, it means last day of the month, i.e. "31st of January" and so on as per the calendar month. It can be used with an offset value, like "L-3", which denotes the "third to last day of the calendar month". In \&lt;day-of-week&gt;, it specifies the "last day of a week". It can also be used with another value in \&lt;day-of-week&gt;, like "6L", which denotes the "last Friday".
	- `W` (weekday) determines the weekday (Monday to Friday) nearest to a given day of the month. For example, if we specify "10W" in the \&lt;day-of-month&gt; field, it means the "weekday near to 10th of that month". So if "10th" is a Saturday, the job will be triggered on "9th," and if "10th" is a Sunday, it will trigger on "11th". If we specify "1W" in \&lt;day-of-month&gt; and if "1st" is Saturday, the job will be triggered on "3rd," which is Monday, and it will not jump back to the previous month.
	- `#` specifies the "N-th" occurrence of a weekday of the month, for example, "third Friday of the month" can be indicated as "6#3". -->
<!-- 	- *Cron Special Strings*
		- @reboot – run once at the startup
		- @yearly or @annualy – run once a year
		- @monthly – run once a month
		- @weekly – run once a week
		- @daily or @midnight – run once a day
		- @hourly – run hourly -->
</details>
</div><div class="alert alert-info">
<p>Cron expressions are historically used to schedule tasks in Apache Airflow.</p>
</div><div class="alert alert-info">
<p>You can use the website <a href="https://crontab.guru/" target="_blank" rel="noopener">https://crontab.guru/</a> to check your cron expressions.</p>
</div><h2 id="Airflow-DAG-APIs"><a class="anchor hidden-xs" href="#Airflow-DAG-APIs" title="Airflow-DAG-APIs"><i class="fa fa-link"></i></a>Airflow DAG APIs</h2><p>Apache Airflow provides two APIs for creating DAGs. The legacy API uses operators (<code>airflow.operators</code>) and DAG (<code>airflow.DAG</code>) objects without using decorators. <strong>TaskFlow API</strong> (<code>airflow.decorators</code>) is the new <strong>recommended</strong> way to create dags where dags and tasks are written using decorators (<code>@dag</code> and <code>@task</code>).</p><div class="alert alert-info">
<p>We can mix those APIs in the same DAG. For instance, you can use TaskFlow API for creating one of the tasks and the traditional API for another task defined both in a DAG which is created using the traditional/new API.</p>
<p>For example, the <a href="#Hello-world-workflow">Hello World Workflow</a> is a DAG created using the traditional API, where the first task is created using traditional  API and the second task is created using TaskFlow API.</p>
</div><div class="alert alert-info">
<p>DAG definition files are python files (<code>.py</code>) but should be stored in a location where Airflow can see it. This location can be determined in <code>airflow.cfg</code> as follows:</p>
<pre><code class="yaml hljs"><span class="hljs-comment"># line 7</span>
<span class="hljs-string">dags_folder</span> <span class="hljs-string">=</span> <span class="hljs-string">/home/firasj/project/services/airflow/dags</span>
</code></pre>
</div><div class="alert alert-warning">
<p>Steps need to be considered when building a pipeline and writing its DAG definition file:</p>
<ol>
<li>Declare the DAG function with required parameters.</li>
<li>Declare the task functions with required parameters.</li>
<li>Call the task functions inside the DAG.</li>
<li>Specify the dependency between the tasks.</li>
<li>Call the DAG function.</li>
<li>Check that you set a <code>schedule</code> and <code>start_date</code>for the DAG.</li>
<li>Check that you set <code>dag_id</code> for the dag and <code>task_id</code> for each task.</li>
<li>Check if you want <code>catchup</code> for past DAG runs.</li>
<li>Go to Airflow web UI to track the workflow/DAG and check if there are import errors.</li>
</ol>
</div><h2 id="DAG-Examples"><a class="anchor hidden-xs" href="#DAG-Examples" title="DAG-Examples"><i class="fa fa-link"></i></a>DAG Examples</h2><p>We have three common ways to write DAG definition files in Airflow. We can define the DAG using:</p><h3 id="1-Traditional-API-as-a-variable"><a class="anchor hidden-xs" href="#1-Traditional-API-as-a-variable" title="1-Traditional-API-as-a-variable"><i class="fa fa-link"></i></a><strong>1. Traditional API as a variable</strong></h3><pre><code class="python hljs"><span class="hljs-comment"># pipelines/hello_dag1.py</span>

<span class="hljs-keyword">from</span> pendulum <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> PythonOperator

<span class="hljs-comment"># A DAG represents a workflow, a collection of tasks</span>
<span class="hljs-comment"># It is a variable</span>
dag = DAG(dag_id=<span class="hljs-string">"hello_dag1"</span>,
          start_date=datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
          schedule=<span class="hljs-literal">None</span>,
          catchup=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Tasks here are created via instantating operators</span>

<span class="hljs-comment"># You need to pass the dag for all tasks</span>

<span class="hljs-comment"># Bash task</span>
hello = BashOperator(task_id=<span class="hljs-string">"hello1"</span>, 
                     bash_command=<span class="hljs-string">"echo hello "</span>, 
                     dag = dag)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">msg</span><span class="hljs-params">()</span>:</span>
    print(<span class="hljs-string">"airflow!"</span>)

<span class="hljs-comment"># Python task</span>
msg = PythonOperator(task_id=<span class="hljs-string">"msg1"</span>, 
                     python_callable=msg, 
                     dag=dag)

<span class="hljs-comment"># Set dependencies between tasks</span>
hello &gt;&gt; msg

</code></pre><h3 id="2-Traditional-API-as-a-context"><a class="anchor hidden-xs" href="#2-Traditional-API-as-a-context" title="2-Traditional-API-as-a-context"><i class="fa fa-link"></i></a><strong>2. Traditional API as a context</strong></h3><pre><code class="python hljs"><span class="hljs-comment"># pipelines/hello_dag2.py</span>

<span class="hljs-keyword">from</span> pendulum <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> PythonOperator

<span class="hljs-comment"># A DAG represents a workflow, a collection of tasks</span>
<span class="hljs-comment"># DAG is defined as a context</span>
<span class="hljs-keyword">with</span> DAG(dag_id=<span class="hljs-string">"hello_dag2"</span>,
         start_date=datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
         schedule=<span class="hljs-literal">None</span>,
         catchup=<span class="hljs-literal">False</span>) <span class="hljs-keyword">as</span> dag:

    
    <span class="hljs-comment"># WE do NOT pass dag here</span>

    hello = BashOperator(task_id=<span class="hljs-string">"hello2"</span>, 
                         bash_command=<span class="hljs-string">"echo hello "</span>)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">msg</span><span class="hljs-params">()</span>:</span>
        print(<span class="hljs-string">"airflow!"</span>)

    called_msg = PythonOperator(task_id=<span class="hljs-string">"msg2"</span>, 
                         python_callable=msg)

    
    <span class="hljs-comment"># Set dependencies between tasks</span>
    <span class="hljs-comment"># hello &gt;&gt; msg</span>
    hello.set_downstream(called_msg)
    <span class="hljs-comment"># OR</span>
    <span class="hljs-comment"># msg.set_upstream(hello)</span>

</code></pre><h3 id="3-TaskFlow-API"><a class="anchor hidden-xs" href="#3-TaskFlow-API" title="3-TaskFlow-API"><i class="fa fa-link"></i></a><strong>3. TaskFlow API</strong></h3><pre><code class="python hljs"><span class="hljs-comment"># pipelines/hello_dag3.py</span>


<span class="hljs-keyword">from</span> pendulum <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> dag, task
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> PythonOperator
<span class="hljs-keyword">from</span> airflow.models.baseoperator <span class="hljs-keyword">import</span> chain

<span class="hljs-meta">@dag(</span>
    dag_id=<span class="hljs-string">"hello_dag3"</span>, 
    start_date=datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
    schedule=<span class="hljs-literal">None</span>,
    catchup=<span class="hljs-literal">False</span>
)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_hello</span><span class="hljs-params">()</span>:</span>

    <span class="hljs-comment"># Tasks here are created via instantating operators</span>
    
    <span class="hljs-comment"># Bash task is defined using Traditional API</span>
    <span class="hljs-comment"># There is a decorator @task.bash in Airflow 2.9+ as a replacement for this</span>
    hello = BashOperator(task_id=<span class="hljs-string">"hello3"</span>, bash_command=<span class="hljs-string">"echo hello "</span>)
    
    <span class="hljs-comment"># Python task is defined sing TaskFlow API</span>
<span class="hljs-meta">    @task(</span>
        task_id = <span class="hljs-string">"msg31"</span>
    )
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">msg</span><span class="hljs-params">()</span>:</span>
        <span class="hljs-string">"""Prints a message"""</span>
        print(<span class="hljs-string">"airflow!"</span>)
    
    called_msg = PythonOperator(task_id=<span class="hljs-string">"msg32"</span>, python_callable=msg)

    
        
    <span class="hljs-comment"># Set dependencies between tasks</span>
    <span class="hljs-comment"># hello &gt;&gt; msg</span>
    <span class="hljs-comment"># OR</span>
    <span class="hljs-comment"># hello.set_downstream(msg)</span>
    <span class="hljs-comment"># OR</span>
    <span class="hljs-comment"># msg.set_upstream(hello)</span>
    <span class="hljs-comment"># OR</span>
    chain(hello, msg)

<span class="hljs-comment"># Call the pipeline since it is defined as a function</span>
print_hello()
</code></pre><div class="alert alert-danger">
<p><strong>Note on <code>bash_command</code> in BashOperator:</strong> Add a space after the script name when directly calling a <code>.sh</code> script with the <code>bash_command</code> argument – for example <code>bash_command="my_script.sh "</code>. This is because Airflow tries to load this file and process it as a Jinja template. Example is <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html#jinja-template-not-found" target="_blank" rel="noopener">here</a>.</p>
<p><em>P.S. Jinja templating is web template engine and lets you define your own variables inside of a template with the {% set %} block such that you can set them at runtime in a dynamic manner. Jinja Syntax:</em></p>
<pre><code class="text hljs">{{ ... }} : <span class="hljs-type">delimiter</span> <span class="hljs-keyword">for</span> variables <span class="hljs-keyword">or</span> expressions
{% ... %} : <span class="hljs-type">delimiter</span> <span class="hljs-keyword">for</span> statements such as <span class="hljs-keyword">if</span> <span class="hljs-keyword">or</span> <span class="hljs-keyword">for</span>
{# ... #} : <span class="hljs-type">comment</span>
</code></pre>
<!-- <br> -->
<center>
<p><img src="https://i.imgur.com/tQsUi8h.png" alt="" width="450" height="300" class="md-image md-image"></p>
</center>
<p>Templates reference in Airflow is <a href="https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html" target="_blank" rel="noopener">here</a>.</p>
</div><h3 id="Xcoms-extra-section"><a class="anchor hidden-xs" href="#Xcoms-extra-section" title="Xcoms-extra-section"><i class="fa fa-link"></i></a>Xcoms [extra section]</h3><details>
<pre><code class="python hljs"><span class="hljs-comment"># pipelines/example_xcoms_vars_dag.py</span>

<span class="hljs-keyword">import</span> pendulum

<span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> task
<span class="hljs-keyword">from</span> airflow.models.dag <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.models.xcom_arg <span class="hljs-keyword">import</span> XComArg
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator

value_1 = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
value_2 = {<span class="hljs-string">"a"</span>: <span class="hljs-string">"b"</span>}


<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">push</span><span class="hljs-params">(ti=None)</span>:</span>
    <span class="hljs-string">"""Pushes an XCom without a specific target"""</span>
    ti.xcom_push(key=<span class="hljs-string">"value from pusher 1"</span>, value=value_1)


<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">push_by_returning</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">"""Pushes an XCom without a specific target, just by returning it"""</span>
    <span class="hljs-keyword">return</span> value_2


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_compare_values</span><span class="hljs-params">(pulled_value, check_value)</span>:</span>
    <span class="hljs-keyword">if</span> pulled_value != check_value:
        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f"The two values differ <span class="hljs-subst">{pulled_value}</span> and <span class="hljs-subst">{check_value}</span>"</span>)


<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">puller</span><span class="hljs-params">(pulled_value_2, ti=None)</span>:</span>
    <span class="hljs-string">"""Pull all previously pushed XComs and check if the pushed values match the pulled values."""</span>
    pulled_value_1 = ti.xcom_pull(task_ids=<span class="hljs-string">"push"</span>, key=<span class="hljs-string">"value from pusher 1"</span>)

    _compare_values(pulled_value_1, value_1)
    _compare_values(pulled_value_2, value_2)


<span class="hljs-meta">@task</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">pull_value_from_bash_push</span><span class="hljs-params">(ti=None)</span>:</span>
    bash_pushed_via_return_value = ti.xcom_pull(key=<span class="hljs-string">"return_value"</span>, task_ids=<span class="hljs-string">"bash_push"</span>)
    bash_manually_pushed_value = ti.xcom_pull(key=<span class="hljs-string">"manually_pushed_value"</span>, task_ids=<span class="hljs-string">"bash_push"</span>)
    print(<span class="hljs-string">f"The xcom value pushed by task push via return value is <span class="hljs-subst">{bash_pushed_via_return_value}</span>"</span>)
    print(<span class="hljs-string">f"The xcom value pushed by task push manually is <span class="hljs-subst">{bash_manually_pushed_value}</span>"</span>)


<span class="hljs-keyword">with</span> DAG(
    <span class="hljs-string">"example_xcom"</span>,
    schedule=<span class="hljs-string">"@once"</span>,
    start_date=pendulum.datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
    catchup=<span class="hljs-literal">False</span>,
    tags=[<span class="hljs-string">"example"</span>],
) <span class="hljs-keyword">as</span> dag:
    bash_push = BashOperator(
        task_id=<span class="hljs-string">"bash_push"</span>,
        bash_command=<span class="hljs-string">'echo "bash_push demo"  &amp;&amp; '</span>
        <span class="hljs-string">'echo "Manually set xcom value '</span>
        <span class="hljs-string">'{{ ti.xcom_push(key="manually_pushed_value", value="manually_pushed_value") }}" &amp;&amp; '</span>
        <span class="hljs-string">'echo "value_by_return"'</span>,
    )

    bash_pull = BashOperator(
        task_id=<span class="hljs-string">"bash_pull"</span>,
        bash_command=<span class="hljs-string">'echo "bash pull demo" &amp;&amp; '</span>
        <span class="hljs-string">f'echo "The xcom pushed manually is <span class="hljs-subst">{XComArg(bash_push, key=<span class="hljs-string">"manually_pushed_value"</span>)}</span>" &amp;&amp; '</span>
        <span class="hljs-string">f'echo "The returned_value xcom is <span class="hljs-subst">{XComArg(bash_push)}</span>" &amp;&amp; '</span>
        <span class="hljs-string">'echo "finished"'</span>,
        do_xcom_push=<span class="hljs-literal">False</span>,
    )

    python_pull_from_bash = pull_value_from_bash_push()

    [bash_pull, python_pull_from_bash] &lt;&lt; bash_push

    puller(push_by_returning()) &lt;&lt; push()
</code></pre>
</details><h3 id="Python-tasks-with-virtual-environments"><a class="anchor hidden-xs" href="#Python-tasks-with-virtual-environments" title="Python-tasks-with-virtual-environments"><i class="fa fa-link"></i></a>Python tasks with virtual environments</h3><pre><code class="python hljs"><span class="hljs-comment"># pipelines/example_python_task_venv_dag.py</span>

<span class="hljs-string">"""
Example DAG demonstrating the usage of the TaskFlow API to execute Python functions natively and within a virtual environment.
"""</span>


<span class="hljs-keyword">import</span> sys
<span class="hljs-keyword">import</span> time

<span class="hljs-keyword">import</span> pendulum

<span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> dag, task
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> is_venv_installed

PATH_TO_PYTHON_BINARY = sys.executable


<span class="hljs-meta">@dag(</span>
    dag_id=<span class="hljs-string">"example_python_venv"</span>,
    schedule=<span class="hljs-literal">None</span>,
    start_date=pendulum.datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
    catchup=<span class="hljs-literal">False</span>,
    tags=[<span class="hljs-string">"example"</span>],
)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">example_python_decorator</span><span class="hljs-params">()</span>:</span>
    
    <span class="hljs-comment"># Generate 5 sleeping tasks, sleeping from 0.0 to 0.4 seconds respectively</span>
<span class="hljs-meta">    @task</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_sleeping_function</span><span class="hljs-params">(random_base)</span>:</span>
        <span class="hljs-string">"""This is a function that will run within the DAG execution"""</span>
        time.sleep(random_base)

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>):
        sleeping_task = my_sleeping_function.override(task_id=<span class="hljs-string">f"sleep_for_<span class="hljs-subst">{i}</span>"</span>)(random_base=i / <span class="hljs-number">10</span>)


    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_venv_installed():
        print(<span class="hljs-string">"The virtalenv_python example task requires virtualenv, please install it."</span>)
    <span class="hljs-keyword">else</span>:

<span class="hljs-meta">        @task.virtualenv(</span>
            task_id=<span class="hljs-string">"virtualenv_python"</span>, requirements=[<span class="hljs-string">"colorama==0.4.0"</span>], system_site_packages=<span class="hljs-literal">False</span>
        )
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">callable_virtualenv</span><span class="hljs-params">()</span>:</span>
            <span class="hljs-string">"""
            Example function that will be performed in a virtual environment.

            Importing at the module level ensures that it will not attempt to import the
            library before it is installed.
            """</span>

            <span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep

            <span class="hljs-keyword">from</span> colorama <span class="hljs-keyword">import</span> Back, Fore, Style

            print(Fore.RED + <span class="hljs-string">"some red text"</span>)
            print(Back.GREEN + <span class="hljs-string">"and with a green background"</span>)
            print(Style.DIM + <span class="hljs-string">"and in dim text"</span>)
            print(Style.RESET_ALL)
            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>):
                print(Style.DIM + <span class="hljs-string">"Please wait..."</span>, flush=<span class="hljs-literal">True</span>)
                sleep(<span class="hljs-number">1</span>)
            print(<span class="hljs-string">"Finished"</span>)

        virtualenv_task = callable_virtualenv()

        sleeping_task &gt;&gt; virtualenv_task

<span class="hljs-meta">        @task.external_python(task_id="external_python", python=PATH_TO_PYTHON_BINARY)</span>
        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">callable_external_python</span><span class="hljs-params">()</span>:</span>
            <span class="hljs-string">"""
            Example function that will be performed in a virtual environment.

            Importing at the module level ensures that it will not attempt to import the
            library before it is installed.
            """</span>
            <span class="hljs-keyword">import</span> sys
            <span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> sleep

            print(<span class="hljs-string">f"Running task via <span class="hljs-subst">{sys.executable}</span>"</span>)
            print(<span class="hljs-string">"Sleeping"</span>)
            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>):
                print(<span class="hljs-string">"Please wait..."</span>, flush=<span class="hljs-literal">True</span>)
                sleep(<span class="hljs-number">1</span>)
            print(<span class="hljs-string">"Finished"</span>)

        external_python_task = callable_external_python()


        external_python_task &gt;&gt; virtualenv_task


dag = example_python_decorator()


<span class="hljs-comment"># Test the pipeline by running </span>
<span class="hljs-comment"># python pipelines/example_python_task_venv_dag.py</span>
<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:
    dag.test()
</code></pre><h3 id="ExternalTaskSensor"><a class="anchor hidden-xs" href="#ExternalTaskSensor" title="ExternalTaskSensor"><i class="fa fa-link"></i></a>ExternalTaskSensor</h3><pre><code class="python hljs"><span class="hljs-comment"># pipelines/example_external_task_sensor_dag.py</span>

<span class="hljs-keyword">import</span> pendulum

<span class="hljs-keyword">from</span> airflow.models.dag <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.empty <span class="hljs-keyword">import</span> EmptyOperator
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator
<span class="hljs-keyword">from</span> airflow.sensors.external_task <span class="hljs-keyword">import</span> ExternalTaskMarker, ExternalTaskSensor
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta
<span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> task

start_date = pendulum.datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">6</span>, <span class="hljs-number">27</span>, <span class="hljs-number">19</span>, <span class="hljs-number">30</span>, tz = <span class="hljs-string">"Europe/Moscow"</span>)

<span class="hljs-keyword">with</span> DAG(
    dag_id=<span class="hljs-string">"example_external_task_sensor_parent"</span>,
    start_date=start_date,
    catchup=<span class="hljs-literal">False</span>,
    schedule=timedelta(minutes=<span class="hljs-number">1</span>),
    tags=[<span class="hljs-string">"example2"</span>],
) <span class="hljs-keyword">as</span> parent_dag:
    
    parent_task = BashOperator(
        task_id = <span class="hljs-string">"parent_task"</span>,
        bash_command=<span class="hljs-string">"echo Run this before! "</span>,
        cwd=<span class="hljs-string">"/"</span>
    )
    

<span class="hljs-keyword">with</span> DAG(
    dag_id=<span class="hljs-string">"example_external_task_sensor_child"</span>,
    start_date=start_date,
    schedule=timedelta(minutes=<span class="hljs-number">1</span>),
    catchup=<span class="hljs-literal">False</span>,
    tags=[<span class="hljs-string">"example2"</span>],
) <span class="hljs-keyword">as</span> child_dag:

    child_task1 = ExternalTaskSensor(
        task_id=<span class="hljs-string">"child_task1"</span>,
        external_dag_id=parent_dag.dag_id,
        external_task_id=parent_task.task_id,
        timeout=<span class="hljs-number">600</span>
    )

<span class="hljs-meta">    @task(task_id = "child_task2")</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_this_after</span><span class="hljs-params">()</span>:</span>
        print(<span class="hljs-string">"I am running!"</span>)
    
    child_task2 = run_this_after()


    child_task1 &gt;&gt; child_task2
</code></pre><h3 id="Trigger-Dag-Run-operator"><a class="anchor hidden-xs" href="#Trigger-Dag-Run-operator" title="Trigger-Dag-Run-operator"><i class="fa fa-link"></i></a>Trigger Dag Run operator</h3><p>It triggers a DAG run for a specified <code>dag_id</code>.</p><pre><code class="python hljs"><span class="hljs-comment"># pipelines/example_trigger_controller_dag.py</span>

<span class="hljs-keyword">import</span> pendulum

<span class="hljs-keyword">from</span> airflow.models.dag <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.trigger_dagrun <span class="hljs-keyword">import</span> TriggerDagRunOperator

<span class="hljs-keyword">with</span> DAG(
    dag_id=<span class="hljs-string">"example_trigger_controller_dag"</span>,
    start_date=pendulum.datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
    catchup=<span class="hljs-literal">False</span>,
    schedule=<span class="hljs-string">"@once"</span>,
    tags=[<span class="hljs-string">"example"</span>],
) <span class="hljs-keyword">as</span> dag:
    trigger = TriggerDagRunOperator(
        task_id=<span class="hljs-string">"test_trigger_dagrun"</span>,
        <span class="hljs-comment"># Ensure this equals the dag_id of the DAG to trigger</span>
        trigger_dag_id=<span class="hljs-string">"example_trigger_target_dag"</span>,  
    )
</code></pre><pre><code class="python hljs"><span class="hljs-comment"># pipelines/example_trigger_target_dag.py</span>

<span class="hljs-keyword">import</span> pendulum

<span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> task
<span class="hljs-keyword">from</span> airflow.models.dag <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.bash <span class="hljs-keyword">import</span> BashOperator


<span class="hljs-meta">@task(task_id="run_this")</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_this_func</span><span class="hljs-params">(dag_run=None)</span>:</span>
    <span class="hljs-string">"""
    Print the payload "message" passed to the DagRun conf attribute.

    :param dag_run: The DagRun object
    """</span>
    print(<span class="hljs-string">"triggerred task!"</span>)


<span class="hljs-keyword">with</span> DAG(
    dag_id=<span class="hljs-string">"example_trigger_target_dag"</span>,
    start_date=pendulum.datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, tz=<span class="hljs-string">"UTC"</span>),
    catchup=<span class="hljs-literal">False</span>,
    schedule=<span class="hljs-literal">None</span>,
    tags=[<span class="hljs-string">"example"</span>],
) <span class="hljs-keyword">as</span> dag:
    run_this = run_this_func()

    bash_task = BashOperator(
        task_id=<span class="hljs-string">"bash_task"</span>,
        bash_command=<span class="hljs-string">'sleep 60 &amp;&amp; echo "Run this after the target"'</span>
    )
</code></pre><div class="alert alert-info">
<p><strong>Info:</strong> You can check more DAG examples from <a href="https://github.com/apache/airflow/tree/main/airflow/example_dags" target="_blank" rel="noopener">here</a>.</p>
</div><h2 id="Deferrable-operators-Extra-section"><a class="anchor hidden-xs" href="#Deferrable-operators-Extra-section" title="Deferrable-operators-Extra-section"><i class="fa fa-link"></i></a>Deferrable operators [Extra section]</h2><details open="">
<ul>
<li>They use the Python <code>asyncio</code> library to run tasks waiting for an external resource to finish (asynchronous run).</li>
<li>This frees up your workers and allows you to use resources more effectively.</li>
<li><em>Triggers</em>: Small, asynchronous sections of Python code. Due to their asynchronous nature, they coexist efficiently in a single process known as the triggerer which runs an asyncio event loop in your Airflow environment.</li>
<li>Running a <em>triggerer</em> is essential for using deferrable operators.</li>
<li>For example, the <code>DateTimeSensorAsync</code> waits asynchronously for a specific date and time to occur.</li>
<li>Note that your Airflow environment needs to run a <em>triggerer</em> component to use deferrable operators.</li>
<li><strong>Deferred task</strong>: An Airflow task state indicating that a task has paused its execution, released the worker slot, and submitted a trigger to be picked up by the triggerer process.</li>
<li>With traditional operators, worker slots are occupied, tasks are queued and start times are delayed.<br>
<img src="https://www.astronomer.io/docs/assets/images/classic_worker_process-a5875f409a23ad1769be12aa51c13eb8.png" alt="" class="md-image md-image"></li>
<li>With deferrable operators, worker slots are released when a task is polling for the job status. When the task is deferred, the polling process is offloaded as a trigger to the triggerer, and the worker slot becomes available. The triggerer can run many asynchronous polling tasks concurrently, and this prevents polling tasks from occupying your worker resources. When the terminal status for the job is received, the task resumes, taking a worker slot while it finishes. The following image illustrates this process:<br>
<img src="https://www.astronomer.io/docs/assets/images/deferrable_operator_process-684cf9e1955514cc33f4801afbace2a6.png" alt="" class="md-image md-image"></li>
<li>Deferrable operators should be used whenever you have tasks that occupy a worker slot while polling for a condition in an external system. For example, using deferrable operators for sensor tasks can provide efficiency gains and reduce operational costs.</li>
<li>Many Airflow operators, such as the <code>TriggerDagRunOperator</code>, can be set to run in deferrable mode using the <code>deferrable</code> parameter.</li>
</ul>
<p><strong>Example</strong></p>
<p>The following example DAG is scheduled to run every minute between its <code>start_date</code> and its <code>end_date</code>. Every DAG run contains one sensor task that will potentially take up to 20 minutes to complete.</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> dag
<span class="hljs-keyword">from</span> airflow.sensors.date_time <span class="hljs-keyword">import</span> DateTimeSensor
<span class="hljs-keyword">from</span> pendulum <span class="hljs-keyword">import</span> datetime


<span class="hljs-meta">@dag(</span>
    start_date=datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">5</span>, <span class="hljs-number">23</span>, <span class="hljs-number">20</span>, <span class="hljs-number">0</span>),
    end_date=datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">5</span>, <span class="hljs-number">23</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>),
    schedule=<span class="hljs-string">"* * * * *"</span>,
    catchup=<span class="hljs-literal">True</span>,
)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sync_dag_2</span><span class="hljs-params">()</span>:</span>
    DateTimeSensor(
        task_id=<span class="hljs-string">"sync_task"</span>,
        target_time=<span class="hljs-string">"""{{ macros.datetime.utcnow() + macros.timedelta(minutes=20) }}"""</span>,
    )


sync_dag_2()
</code></pre>
<p><img src="https://www.astronomer.io/docs/assets/images/standard_sensor_slot_taking-71956444ab01fcbd0bf2c8f356cee9f7.png" alt="" class="md-image md-image"></p>
<p><strong>Note:</strong> Using <code>DateTimeSensor</code>, one worker slot is taken up by every sensor that runs.</p>
<p>By using the deferrable version of this sensor, <code>DateTimeSensorAsync</code>, you can achieve full concurrency while freeing up your workers to complete additional tasks across your Airflow environment.</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> airflow.decorators <span class="hljs-keyword">import</span> dag
<span class="hljs-keyword">from</span> pendulum <span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> airflow.sensors.date_time <span class="hljs-keyword">import</span> DateTimeSensorAsync


<span class="hljs-meta">@dag(</span>
    start_date=datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">5</span>, <span class="hljs-number">23</span>, <span class="hljs-number">20</span>, <span class="hljs-number">0</span>),
    end_date=datetime(<span class="hljs-number">2024</span>, <span class="hljs-number">5</span>, <span class="hljs-number">23</span>, <span class="hljs-number">20</span>, <span class="hljs-number">19</span>),
    schedule=<span class="hljs-string">"* * * * *"</span>,
    catchup=<span class="hljs-literal">True</span>,
)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">async_dag_2</span><span class="hljs-params">()</span>:</span>
    DateTimeSensorAsync(
        task_id=<span class="hljs-string">"async_task"</span>,
        target_time=<span class="hljs-string">"""{{ macros.datetime.utcnow() + macros.timedelta(minutes=20) }}"""</span>,
    )


async_dag_2()
</code></pre>
<p><img src="https://www.astronomer.io/docs/assets/images/deferrable_grid_view-6af110dce8fdef5e74522c1742b1880f.png" alt="" class="md-image md-image"></p>
<p>In the previous screenshot, all tasks are shown in a deferred (violet) state. Tasks in other DAGs can use the available worker slots, making the deferrable operator more cost and time-efficient.</p>
</details><h2 id="Test-a-pipeline"><a class="anchor hidden-xs" href="#Test-a-pipeline" title="Test-a-pipeline"><i class="fa fa-link"></i></a>Test a pipeline</h2><ul>
<li>You can test a pipeline from airflow CLI as follows:</li>
</ul><pre><code class="python hljs">airflow dags test &lt;dag-id&gt;
</code></pre><p>This will run the dag only for one time. This is not scheduling dags.</p><ul>
<li>You can also test it in VS code by adding the code snippet below to the end of the dag definition file:</li>
</ul><pre><code class="python hljs">

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    dag.test()
    <span class="hljs-comment"># dag is an instance of DAG</span>
</code></pre><h2 id="Airflow-UI"><a class="anchor hidden-xs" href="#Airflow-UI" title="Airflow-UI"><i class="fa fa-link"></i></a>Airflow UI</h2><p>The Airflow UI makes it easy to monitor and troubleshoot your data pipelines. Here’s a quick overview of some of the features and visualizations you can find in the Airflow UI.</p><h3 id="DAGs-View"><a class="anchor hidden-xs" href="#DAGs-View" title="DAGs-View"><i class="fa fa-link"></i></a>DAGs View</h3><p>List of the DAGs in your environment, and a set of shortcuts to useful pages. You can see exactly how many tasks succeeded, failed, or are currently running at a glance.<br>
<img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/dags.png" alt="" class="md-image md-image"></p><h3 id="Cluster-Activity-View"><a class="anchor hidden-xs" href="#Cluster-Activity-View" title="Cluster-Activity-View"><i class="fa fa-link"></i></a>Cluster Activity View</h3><p>Native Airflow dashboard page into the UI to collect several useful metrics for monitoring your Airflow cluster.<br>
<img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/cluster_activity.png" alt="" class="md-image md-image"></p><h3 id="Datasets-View"><a class="anchor hidden-xs" href="#Datasets-View" title="Datasets-View"><i class="fa fa-link"></i></a>Datasets View</h3><p>A combined listing of the current datasets and a graph illustrating how they are produced and consumed by DAGs.</p><p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/datasets.png" alt="" class="md-image md-image"></p><h3 id="Grid-View"><a class="anchor hidden-xs" href="#Grid-View" title="Grid-View"><i class="fa fa-link"></i></a>Grid View</h3><p>A bar chart and grid representation of the DAG that spans across time. The top row is a chart of DAG Runs by duration, and below, task instances. If a pipeline is late, you can quickly see where the different steps are and identify the blocking ones.</p><p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/grid.png" alt="" class="md-image md-image"></p><h3 id="Graph-View"><a class="anchor hidden-xs" href="#Graph-View" title="Graph-View"><i class="fa fa-link"></i></a>Graph View</h3><p>The graph view is perhaps the most comprehensive. Visualize your DAG’s dependencies and their current status for a specific run.</p><p><img src="https://airflow.apache.org/docs/apache-airflow/stable/_images/graph.png" alt="" class="md-image md-image"></p><div class="alert alert-info">
<p>The landing time for a task instance is the delta between the dag run’s data interval end (typically this means when the dag “should” run) and the dag run completion time.</p>
</div><h1 id="ZenML"><a class="anchor hidden-xs" href="#ZenML" title="ZenML"><i class="fa fa-link"></i></a>ZenML</h1><p>ZenML is an extensible, open-source MLOps framework for creating portable, production-ready machine learning pipelines.</p><pre><code class="yaml hljs"><span class="hljs-comment"># You do not need to run this if you installed the package from requirements.txt</span>
<span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">zenml[server]</span>
</code></pre><p>This will install ZenML with with the dashboard.</p><div class="alert alert-danger">
<p>Note that ZenML currently supports Python 3.8, 3.9, 3.10, and 3.11. Please make sure that you are using a supported Python version.</p>
</div><div class="alert alert-danger">
<p><strong>Important notes:</strong></p>
<ul>
<li>Before running ZenML pipelines or deploying ZenML stack. We need to customize the ZenML config folder as follows.<pre><code class="yaml hljs"><span class="hljs-comment"># PWD is project folder</span>
<span class="hljs-string">export</span> <span class="hljs-string">ZENML_CONFIG_PATH=$PWD/services/zenml</span>
</code></pre>
</li>
<li>Make sure that you set this environment variable before you work with ZenML. This will store the configs and metadata in the specified folder. This folder needs to be pushed to Github.</li>
<li>You can add this to your <code>~/.bashrc</code> file as a permanent variable but replace <code>$PWD</code> with your absolute project path.<pre><code class="console hljs"><span class="hljs-comment"># REPLACE &lt;project-folder-path&gt; with your project folder path</span>
<span class="hljs-built_in">cd</span> &lt;project-folder-path&gt;
<span class="hljs-built_in">echo</span> <span class="hljs-string">"export ZENML_CONFIG_PATH=<span class="hljs-variable">$PWD</span>/services/zenml"</span> &gt;&gt; ~/.bashrc

<span class="hljs-comment"># Run the file</span>
<span class="hljs-built_in">source</span> ~/.bashrc

<span class="hljs-comment"># Activate the virtual environment again</span>
<span class="hljs-built_in">source</span> .venv/bin/activate
</code></pre>
</li>
</ul>
</div><h2 id="Core-concepts1"><a class="anchor hidden-xs" href="#Core-concepts1" title="Core-concepts1"><i class="fa fa-link"></i></a>Core concepts</h2><h3 id="Step"><a class="anchor hidden-xs" href="#Step" title="Step"><i class="fa fa-link"></i></a>Step</h3><p>Steps are functions annotated with the @step decorator. They represent a single stage to be used in a pipeline.</p><pre><code class="python hljs"><span class="hljs-meta">@step</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step_1</span><span class="hljs-params">()</span> -&gt; str:</span>
    <span class="hljs-string">"""Returns a string."""</span>
    <span class="hljs-keyword">return</span> <span class="hljs-string">"world"</span>
</code></pre><p>These “step” functions can also have <em>inputs and outputs</em>. For ZenML to work properly, these should preferably be typed.</p><pre><code class="python hljs">
<span class="hljs-meta">@step(enable_cache=False)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step_2</span><span class="hljs-params">(input_data: str, input_data2: str)</span> -&gt; str:</span>
    <span class="hljs-string">"""Combines the two strings passed in."""</span>
    output = <span class="hljs-string">f"<span class="hljs-subst">{input_data}</span> <span class="hljs-subst">{input_data2}</span>"</span>
    <span class="hljs-keyword">return</span> output <span class="hljs-comment"># Output</span>
</code></pre><h3 id="Pipelines"><a class="anchor hidden-xs" href="#Pipelines" title="Pipelines"><i class="fa fa-link"></i></a>Pipelines</h3><p>At its core, ZenML follows a pipeline-based workflow for your projects. A <strong>pipeline</strong> consists of a series of <strong>steps</strong>, organized in any order that makes sense for your use case. Pipelines are simple Python functions decorated with <code>@pipeline</code>. It is only allowed to call <em>steps</em> within this function.</p><pre><code class="python hljs"><span class="hljs-meta">@pipeline</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_pipeline</span><span class="hljs-params">()</span>:</span>
    output_step_one = step_1()
    step_2(input_data=<span class="hljs-string">"hello"</span>, input_data2=output_step_one)
</code></pre><p>The inputs for steps called within a pipeline can either be the outputs of previous steps or alternatively, you can pass in values directly (as long as they’re JSON-serializable).</p><p>Executing the Pipeline is as easy as calling the function that you decorated with the <code>@pipeline</code> decorator.</p><pre><code class="python hljs">
<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:
	my_pipeline()
</code></pre><h3 id="Artifacts"><a class="anchor hidden-xs" href="#Artifacts" title="Artifacts"><i class="fa fa-link"></i></a>Artifacts</h3><p>Artifacts represent the data that goes through your steps as inputs and outputs and they are automatically tracked and stored by ZenML in the <em>artifact store</em>.</p><h3 id="Materializers"><a class="anchor hidden-xs" href="#Materializers" title="Materializers"><i class="fa fa-link"></i></a>Materializers</h3><p>A ZenML pipeline is built in a data-centric way. The outputs and inputs of steps define how steps are connected and the order in which they are executed. Each step should be considered as its very own process that reads and writes its inputs and outputs from and to the <em>artifact store</em>.</p><p>A materializer dictates how a given artifact can be written to and retrieved from the artifact store and also contains all serialization and deserialization logic. Whenever you pass artifacts as outputs from one pipeline step to other steps as inputs, the corresponding materializer for the respective data type defines how this artifact is first serialized and written to the artifact store, and then deserialized and read in the next step.</p><p>Check this <a href="https://docs.zenml.io/how-to/handle-data-artifacts/handle-custom-data-types" target="_blank" rel="noopener">page</a> for more info.</p><h2 id="ZenML-Example"><a class="anchor hidden-xs" href="#ZenML-Example" title="ZenML-Example"><i class="fa fa-link"></i></a>ZenML Example</h2><p>Here we will build an ETL pipeline to extract the data from the datastore, transform it, validate it, then load it to the feature store.</p><h3 id="ETL-data-pipeline"><a class="anchor hidden-xs" href="#ETL-data-pipeline" title="ETL-data-pipeline"><i class="fa fa-link"></i></a>ETL data pipeline</h3><p>This pipeline is a typical example of one of the pipelines required for the project. As you can see, you have to write the boilerplate code in <code>src</code> folder/package and here you just call them. Notice the input and output for each step. All the input and ouptut in this pipeline are <strong>materialized and versioned</strong> in an artifact store (SQLite db file locally).</p><div class="alert alert-warning">
<p>You should work on building data pipelines using ZenML and Airflow for the project, as follows:</p>
<ol>
<li>Determine the steps of the pipeline.</li>
<li>Specify the input/output for each step.</li>
<li>For each step, write a function in <code>src/data.py</code> to perform a single step of the pipeline.</li>
<li>Add test function in <code>tests</code> folder to test each function you added in <code>src/data.py</code>. Test the functions.</li>
<li>Call/Use the functions from <code>src/data.py</code> in the steps of the pipeline.</li>
<li>Create and call the pipeline.</li>
<li>Run the pipeline using <code>zenml</code>.
<ul>
<li><code>python pipelines/&lt;file.py&gt;</code></li>
</ul>
</li>
<li>Schedule and orchestrate the pipeline using <code>Apache Airflow</code>.
<ul>
<li>You can create a Bash operator for running the previous command.</li>
</ul>
</li>
</ol>
</div><pre><code class="python hljs"><span class="hljs-comment"># pipelines/data_prepare.py</span>

<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> typing_extensions <span class="hljs-keyword">import</span> Tuple, Annotated
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> step, pipeline, ArtifactConfig
<span class="hljs-keyword">from</span> data <span class="hljs-keyword">import</span> transform_data, extract_data, load_features, validate_transformed_data
<span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> get_sample_version
<span class="hljs-keyword">import</span> os

BASE_PATH = os.path.expandvars(<span class="hljs-string">"$PROJECTPATH"</span>)

<span class="hljs-meta">@step(enable_cache=False)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">extract</span><span class="hljs-params">()</span>-&gt; Tuple[
                Annotated[pd.DataFrame,
                        ArtifactConfig(name="extracted_data", 
                                       tags=["data_preparation"]
                                       )
                        ],
                Annotated[str,
                        ArtifactConfig(name="data_version",
                                       tags=["data_preparation"])]
                    ]:</span>
    
    df, version = extract_data(BASE_PATH)

    <span class="hljs-keyword">return</span> df, version

<span class="hljs-meta">@step(enable_cache=False)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transform</span><span class="hljs-params">(df: pd.DataFrame)</span>-&gt; Tuple[
                    Annotated[pd.DataFrame, 
                            ArtifactConfig(name="input_features",
                                           tags=["data_preparation"])],
                    Annotated[pd.DataFrame,
                            ArtifactConfig(name="input_target", 
                                            tags=["data_preparation"])]
                                    ]:</span>

    <span class="hljs-comment"># Your data transformation code</span>
    X, y = transform_data(df)

    <span class="hljs-keyword">return</span> X, y

<span class="hljs-meta">@step(enable_cache=False)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate</span><span class="hljs-params">(X:pd.DataFrame, 
             y:pd.DataFrame)</span>-&gt;Tuple[
                    Annotated[pd.DataFrame, 
                            ArtifactConfig(name="valid_input_features",
                                           tags=["data_preparation"])],
                    Annotated[pd.DataFrame,
                            ArtifactConfig(name="valid_target",
                                           tags=["data_preparation"])]
                                    ]:</span>

    X, y = validate_transformed_data(X, y)
    
    <span class="hljs-keyword">return</span> X, y


<span class="hljs-meta">@step(enable_cache=False)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load</span><span class="hljs-params">(X:pd.DataFrame, y:pd.DataFrame, version: str)</span>-&gt; Tuple[
                    Annotated[pd.DataFrame, 
                            ArtifactConfig(name="features",
                                           tags=["data_preparation"])],
                    Annotated[pd.DataFrame,
                            ArtifactConfig(name="target",
                                           tags=["data_preparation"])]
                                    ]:</span>
    
    load_features(X, y, version)

    <span class="hljs-keyword">return</span> X, y


<span class="hljs-meta">@pipeline()</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_data_pipeline</span><span class="hljs-params">()</span>:</span>
    df, version = extract()
    X, y = transform(df)
    X, y = validate(X, y)
    X, y = load(X, y, version)


<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">"__main__"</span>:
    run = prepare_data_pipeline()
</code></pre><p>You can run the pipeline as:</p><pre><code class="yaml hljs"><span class="hljs-string">python</span> <span class="hljs-string">pipelines/data_prepare.py</span>
</code></pre><p>OR more specific about pipeline selection:</p><pre><code class="yaml hljs"><span class="hljs-string">python</span> <span class="hljs-string">pipelines/data_prepare.py</span> <span class="hljs-bullet">-prepare_data_pipeline</span>
</code></pre><div class="alert alert-warning">
<p>DEclaring the data types for the input and output of the step functions is important as it determines how the artifact will be materialized and saved in the artifact store.</p>
</div><h2 id="Explore-ZenML-Dashboard"><a class="anchor hidden-xs" href="#Explore-ZenML-Dashboard" title="Explore-ZenML-Dashboard"><i class="fa fa-link"></i></a>Explore ZenML Dashboard</h2><p>Once the pipeline has finished its execution, use the <code>zenml up</code> command to view the results in the ZenML Dashboard. Using that command will open up the browser automatically.</p><pre><code class="yaml hljs"><span class="hljs-comment"># Make sure that you customized the config folder for ZenML</span>
<span class="hljs-string">zenml</span> <span class="hljs-string">up</span>
</code></pre><p>Usually, the dashboard is accessible at (<a href="http://127.0.0.1:8237/" target="_blank" rel="noopener">http://127.0.0.1:8237/</a>). Log in with the default username “default” (password not required) and see your recently pipeline run.</p><p><img src="https://i.imgur.com/p8S9pz3.png" alt="image" class="md-image md-image"></p><p>If you have closed the browser tab with the ZenML dashboard, you can always reopen it by running the following command in your terminal.</p><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">show</span>
</code></pre><h2 id="ZenML-Stacks"><a class="anchor hidden-xs" href="#ZenML-Stacks" title="ZenML-Stacks"><i class="fa fa-link"></i></a>ZenML Stacks</h2><p>A <code>stack</code> is the configuration of tools and infrastructure that your pipelines can run on. When you run ZenML code without configuring a stack, the pipeline will run on the so-called <code>default</code> stack.</p><p><img src="https://docs.zenml.io/~gitbook/image?url=https%3A%2F%2F3114808333-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FyOzvAUqhes0ZU6NRd8qv%252Fuploads%252Fgit-blob-506972ee9e2ae0618aa74e36e95f5b9d725379e0%252F02_pipeline_local_stack.png%3Falt%3Dmedia&amp;width=768&amp;dpr=4&amp;quality=100&amp;sign=529d7636bda60015e2e35c0d4c86340b6df1fdb69e086fef90b4661f50e24c8f" alt="" class="md-image md-image"></p><p>We can see the separation of code from configuration and infrastructure. A stack consists of multiple components. All stacks have at minimum an orchestrator and an artifact store.</p><ul>
<li>Describe ZenML stack</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">stack</span> <span class="hljs-string">describe</span>
</code></pre><ul>
<li>List all stacks</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">stack</span> <span class="hljs-string">list</span>
</code></pre><h3 id="Orchestrator"><a class="anchor hidden-xs" href="#Orchestrator" title="Orchestrator"><i class="fa fa-link"></i></a>Orchestrator</h3><p>The orchestrator is responsible for executing the pipeline code. In the simplest case, this will be a simple Python thread on your machine. Let’s explore this default orchestrator.</p><ul>
<li>To list all orchestrators that are registered in your zenml deployment.</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">orchestrator</span> <span class="hljs-string">list</span>
</code></pre><h3 id="Artifact-store"><a class="anchor hidden-xs" href="#Artifact-store" title="Artifact-store"><i class="fa fa-link"></i></a>Artifact store</h3><p>The artifact store is responsible for persisting the step outputs.</p><ul>
<li>List the artifact stores</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact-store</span> <span class="hljs-string">list</span>
</code></pre><ul>
<li>List all flavors of artifact stores</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact-store</span> <span class="hljs-string">flavor</span> <span class="hljs-bullet">--list</span>
</code></pre><div class="alert alert-warning">
<p>You can use ZenML API to get a specific version of the artifact:</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> zenml.client <span class="hljs-keyword">import</span> Client

client = Client()

<span class="hljs-comment"># data: pd.DataFrame (Materialized data)</span>
data = client.get_artifact_version(name_id_or_prefix=<span class="hljs-string">"initial_dataframe"</span>, version=<span class="hljs-number">4</span>).load()

<span class="hljs-comment"># This will retrieve the version 4 of the artifact named `initial_dataframe`. This name is the same when we defined annotated output for the first step of the previous data pipeline.</span>

print(data.shape)
</code></pre>
</div><h4 id="Register-a-new-artifact-store-extra-section"><a class="anchor hidden-xs" href="#Register-a-new-artifact-store-extra-section" title="Register-a-new-artifact-store-extra-section"><i class="fa fa-link"></i></a>Register a new artifact store [extra section]</h4><p>You can create a new artifact store by running the following command:</p><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact-store</span> <span class="hljs-string">register</span> <span class="hljs-string">my_artifact_store</span> <span class="hljs-bullet">--flavor=local</span> 
</code></pre><p>You can describe your new artifact store as follows:</p><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact-store</span> <span class="hljs-string">describe</span> <span class="hljs-string">my_artifact_store</span>
</code></pre><h4 id="Artifact-store-API"><a class="anchor hidden-xs" href="#Artifact-store-API" title="Artifact-store-API"><i class="fa fa-link"></i></a>Artifact store API</h4><div class="alert alert-warning">
<p>When calling the Artifact Store API, you should always use URIs that are relative to the Artifact Store root path.</p>
</div><pre><code class="python hljs"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> zenml.client <span class="hljs-keyword">import</span> Client
<span class="hljs-keyword">from</span> zenml.io <span class="hljs-keyword">import</span> fileio

<span class="hljs-comment"># Artifact Store root path</span>
root_path = Client().active_stack.artifact_store.path

<span class="hljs-comment"># create a custom artifact and store in artifact store</span>
artifact_contents = <span class="hljs-string">"example artifact"</span>
artifact_path = os.path.join(root_path, <span class="hljs-string">"artifacts"</span>, <span class="hljs-string">"examples"</span>)
artifact_uri = os.path.join(artifact_path, <span class="hljs-string">"test.txt"</span>)
fileio.makedirs(artifact_path)
<span class="hljs-keyword">with</span> fileio.open(artifact_uri, <span class="hljs-string">"w"</span>) <span class="hljs-keyword">as</span> f:
    f.write(artifact_contents)
</code></pre><h3 id="Create-a-local-stack-extra-section"><a class="anchor hidden-xs" href="#Create-a-local-stack-extra-section" title="Create-a-local-stack-extra-section"><i class="fa fa-link"></i></a>Create a local stack [extra section]</h3><p>With the artifact store created, we can now create a new stack with this artifact store.</p><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">stack</span> <span class="hljs-string">register</span> <span class="hljs-string">my_local_stack</span> <span class="hljs-bullet">-o</span> <span class="hljs-string">default</span> <span class="hljs-bullet">-a</span> <span class="hljs-string">my_artifact_store</span>
</code></pre><p>To run a pipeline using the new stack:</p><ol>
<li>set the stack as active on your client</li>
</ol><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">stack</span> <span class="hljs-string">set</span> <span class="hljs-string">my_local_stack</span>
</code></pre><ol start="2">
<li>Run your pipeline code</li>
</ol><pre><code class="yaml hljs"><span class="hljs-comment"># Initiates a new run for the pipeline: prepare_data_pipeline</span>
<span class="hljs-string">python</span> <span class="hljs-string">pipelines/prepare_data_pipeline.py</span> <span class="hljs-bullet">-prepare_data_pipeline</span>
</code></pre><h2 id="Manage-artifacts-in-ZenML"><a class="anchor hidden-xs" href="#Manage-artifacts-in-ZenML" title="Manage-artifacts-in-ZenML"><i class="fa fa-link"></i></a>Manage artifacts in ZenML</h2><p>ZenML takes a proactive approach to data versioning, ensuring that every artifact—be it data, models, or evaluations—is automatically tracked and versioned upon pipeline execution.</p><h3 id="Adding-artifacts"><a class="anchor hidden-xs" href="#Adding-artifacts" title="Adding-artifacts"><i class="fa fa-link"></i></a>Adding artifacts</h3><p>ZenML automatically versions all created artifacts using auto-incremented numbering. Yo can use <code>ArtifactConfig</code> to add version and metadata to the artifacts.</p><pre><code class="python hljs"><span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> step, get_step_context, ArtifactConfig
<span class="hljs-keyword">from</span> typing_extensions <span class="hljs-keyword">import</span> Annotated
    
<span class="hljs-comment"># below we combine both approaches, so the artifact will get</span>
<span class="hljs-comment"># metadata and tags from both sources</span>
<span class="hljs-meta">@step</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_data_loader</span><span class="hljs-params">()</span> -&gt; (
    Annotated[
        str,
        ArtifactConfig(
            name="artifact_name",
            version="version_number",
            run_metadata={"metadata_key":</span> <span class="hljs-string">"metadata_value"</span>},
            tags=[<span class="hljs-string">"tag_name"</span>],
        ),
    ]
):
    step_context = get_step_context()
    step_context.add_output_metadata(
        output_name=<span class="hljs-string">"artifact_name"</span>, metadata={<span class="hljs-string">"metadata_key2"</span>: <span class="hljs-string">"metadata_value2"</span>}
    )
    step_context.add_output_tags(output_name=<span class="hljs-string">"artifact_name"</span>, tags=[<span class="hljs-string">"tag_name2"</span>])
    <span class="hljs-keyword">return</span> <span class="hljs-string">"string"</span>
</code></pre><ul>
<li>List the artifacts</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact</span> <span class="hljs-string">list</span>
</code></pre><ul>
<li>List the artifact version</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact</span> <span class="hljs-string">version</span> <span class="hljs-string">list</span>
</code></pre><ul>
<li>List ther artifact version of a spcific artifact “valid_input_dataframe”</li>
</ul><pre><code class="yaml hljs"><span class="hljs-string">zenml</span> <span class="hljs-string">artifact</span> <span class="hljs-string">version</span> <span class="hljs-string">list</span> <span class="hljs-bullet">--name</span> <span class="hljs-string">valid_input_dataframe</span>
</code></pre><h3 id="Consuming-artifacts"><a class="anchor hidden-xs" href="#Consuming-artifacts" title="Consuming-artifacts"><i class="fa fa-link"></i></a>Consuming artifacts</h3><h4 id="Within-the-pipeline"><a class="anchor hidden-xs" href="#Within-the-pipeline" title="Within-the-pipeline"><i class="fa fa-link"></i></a>Within the pipeline</h4><pre><code class="python hljs"><span class="hljs-keyword">from</span> uuid <span class="hljs-keyword">import</span> UUID
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> step, pipeline
<span class="hljs-keyword">from</span> zenml.client <span class="hljs-keyword">import</span> Client


<span class="hljs-meta">@step</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_data</span><span class="hljs-params">(dataset: pd.DataFrame)</span>:</span>
    ...

<span class="hljs-meta">@pipeline</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">training_pipeline</span><span class="hljs-params">()</span>:</span>
    client = Client()
    <span class="hljs-comment"># Fetch by ID</span>
    dataset_artifact = client.get_artifact_version(
        name_id_or_prefix=UUID(<span class="hljs-string">"3a92ae32-a764-4420-98ba-07da8f742b76"</span>)
    )

    <span class="hljs-comment"># Fetch by name alone - uses the latest version of this artifact</span>
    dataset_artifact = client.get_artifact_version(name_id_or_prefix=<span class="hljs-string">"iris_dataset"</span>)

    <span class="hljs-comment"># Fetch by name and version</span>
    dataset_artifact = client.get_artifact_version(
        name_id_or_prefix=<span class="hljs-string">"iris_dataset"</span>, version=<span class="hljs-string">"raw_2023"</span>
    )

    <span class="hljs-comment"># Pass into any step</span>
    process_data(dataset=dataset_artifact)


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    training_pipeline()
</code></pre><h4 id="Outside-the-pipeline"><a class="anchor hidden-xs" href="#Outside-the-pipeline" title="Outside-the-pipeline"><i class="fa fa-link"></i></a>Outside the pipeline</h4><pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> zenml <span class="hljs-keyword">import</span> ExternalArtifact, pipeline, step, save_artifact, load_artifact

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span><span class="hljs-params">(df)</span>:</span>
    save_artifact(df, name=<span class="hljs-string">"df_dataframe"</span>)

<span class="hljs-meta">@step</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">print_data</span><span class="hljs-params">(data: np.ndarray)</span>:</span>
    print(data)

<span class="hljs-meta">@pipeline</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">printing_pipeline</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># One can also pass data directly into the ExternalArtifact</span>
    <span class="hljs-comment"># to create a new artifact on the fly</span>
    data = ExternalArtifact(value=np.array([<span class="hljs-number">0</span>]))

    print_data(data=data)
    load_artifact(name=<span class="hljs-string">"df_dataframe"</span>)


<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    printing_pipeline()
</code></pre><h2 id="Schedule-ZenML-pipelines-using-Airflow"><a class="anchor hidden-xs" href="#Schedule-ZenML-pipelines-using-Airflow" title="Schedule-ZenML-pipelines-using-Airflow"><i class="fa fa-link"></i></a>Schedule ZenML pipelines using Airflow</h2><p>We can run the ZenML pipelines (<code>pipelines/data_prepare.py</code>) by simply running it as follows:</p><pre><code class="yaml hljs"><span class="hljs-string">python</span> <span class="hljs-string">pipelines/prepare_data_pipeline.py</span> <span class="hljs-bullet">-prepare_data_pipeline</span>
</code></pre><p>We can consider this as a bash command and use BashOperator or @task.bash to run the pipeline as follows:</p><pre><code class="python hljs">...

<span class="hljs-comment"># REPLACE &lt;project-folder-path&gt;</span>
zenml_pipeline = BashOperator(
        task_id=<span class="hljs-string">"run_zenml_pipeline"</span>,
        bash_command=<span class="hljs-string">"python &lt;project-folder-path&gt;/services/airflow/dags/data_prepare.py -prepare_data_pipeline "</span>,
        cwd=<span class="hljs-string">"&lt;project-folder-path&gt;/project"</span>, <span class="hljs-comment"># specifies the current working directory</span>
    )
...
</code></pre><div class="alert alert-warning">
<p>Do not run the ZenML pipeline by simply calling the pipeline function using a PythonOperator.</p>
</div><h1 id="Feast-Extra-section"><a class="anchor hidden-xs" href="#Feast-Extra-section" title="Feast-Extra-section"><i class="fa fa-link"></i></a>Feast [Extra section]</h1><details>
<p>Feast is a standalone, open-source <em>feature store</em> that organizations use to store and serve features consistently for <em>offline</em> training and <em>online</em> inference. Feast allows to:</p>
<ul>
<li>Make features consistently available for training and serving by managing an <strong>offline store</strong> (to process historical data for scale-out batch scoring or model training), a low-latency <strong>online store</strong> (to power real-time prediction), and a battle-tested <strong>feature server</strong> (to serve pre-computed features online).</li>
<li>Avoid data leakage by generating point-in-time correct feature sets so data scientists can focus on feature engineering rather than debugging error-prone dataset joining logic. This ensure that future feature values do not leak to models during training.</li>
<li>Decouple ML from data infrastructure by providing a single data access layer that abstracts feature storage from feature retrieval, ensuring models remain portable as you move from training models to serving models.</li>
</ul>
<p><img src="https://feastsite.wpenginepowered.com/wp-content/uploads/2023/01/feast-home-hero@4x.png" alt="" class="md-image md-image"></p>
<p><strong>Note:</strong> Feast today primarily addresses <strong>timestamped structured</strong> data.</p>
<p><strong>Note:</strong> If your data in the dataframe does not have timestamps, you need to create dummy timestamps and add them to the dataframe in order to persist it in the feature store.</p>
<h2 id="Core-concepts2"><a class="anchor hidden-xs" href="#Core-concepts2" title="Core-concepts2"><i class="fa fa-link"></i></a>Core concepts</h2>
<p>The top-level namespace within Feast is a <strong>project</strong>. Users define one or more feature views within a project. Each <strong>feature view</strong> contains one or more <strong>features</strong>. These features typically relate to one or more <strong>entities</strong>. A feature view must always have a <strong>data source</strong>, which in turn is used during the generation of training datasets and when materializing feature values into the online store.</p>
<p><img src="https://docs.feast.dev/~gitbook/image?url=https%3A%2F%2F1077803379-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252FkZAY3SRpOLt8j3A1DKAA%252Fuploads%252Fgit-blob-af58d3cf3809fcc5e69119de273668f715f27538%252Fimage%2520%287%29.png%3Falt%3Dmedia&amp;width=768&amp;dpr=4&amp;quality=100&amp;sign=6673daf57a0fc0f5cef2b6a31c8a8089c62f1aa142da01c797e6a056ce73e073" alt="" class="md-image md-image"></p>
<h3 id="Install-feast"><a class="anchor hidden-xs" href="#Install-feast" title="Install-feast"><i class="fa fa-link"></i></a>Install feast</h3>
<pre><code class="yaml hljs"><span class="hljs-comment"># You do not need to run this if you installed the package from requirements.txt</span>
<span class="hljs-string">pip</span> <span class="hljs-string">install</span> <span class="hljs-string">feast</span>
</code></pre>
<p><strong>Note:</strong> feast 0.36.0+ may work with Windows but it has conflicts with other packages in our toolset. The earlier versions of feast usually do not work on Windows. If you try to run, you may see such errors <code>fcntl module not found</code>. Windows users are working on Ubuntu WSL2 and no issues will be encountered I presume.</p>
<h2 id="Demo1"><a class="anchor hidden-xs" href="#Demo1" title="Demo1"><i class="fa fa-link"></i></a>Demo</h2>
<p>Here we will build a local feature store to store the ML-ready dataset.</p>
<h3 id="1-Create-a-feature-repository"><a class="anchor hidden-xs" href="#1-Create-a-feature-repository" title="1-Create-a-feature-repository"><i class="fa fa-link"></i></a>1. Create a feature repository</h3>
<pre><code class="yaml hljs"><span class="hljs-comment"># Create a directory for feast metadata</span>
<span class="hljs-string">mkdir</span> <span class="hljs-bullet">-p</span> <span class="hljs-string">services/feast</span>

<span class="hljs-comment"># Access the folder</span>
<span class="hljs-comment"># cd services/feast</span>

<span class="hljs-comment"># feast_project should be a name and they do not support path</span>
<span class="hljs-string">feast</span> <span class="hljs-bullet">-c</span> <span class="hljs-string">services/feast</span> <span class="hljs-string">init</span> <span class="hljs-bullet">-t</span> <span class="hljs-string">local</span> <span class="hljs-bullet">-m</span> <span class="hljs-string">feast_project</span>

<span class="hljs-comment"># Access the feast repo</span>
<span class="hljs-string">cd</span> <span class="hljs-string">feast_project/feature_repo</span>
</code></pre>
<p>The repo has <code>feature_store.yaml</code> which contains a demo setup configuring where data sources are.</p>
<!-- test_workflow.py showcases how to run all key Feast commands, including defining, retrieving, and pushing features. You can run this with python test_workflow.py -->
<h3 id="2-Configure-the-feature-repository"><a class="anchor hidden-xs" href="#2-Configure-the-feature-repository" title="2-Configure-the-feature-repository"><i class="fa fa-link"></i></a>2. Configure the feature repository</h3>
<p>The configuration of the feature repository is in the location <code>services\feast\feast_project\feature_repo\feature_store.yaml</code>. We can configure it as follows:</p>
<pre><code class="yaml hljs"><span class="hljs-attr">project:</span> <span class="hljs-string">feast_project</span>
<span class="hljs-attr">registry:</span> <span class="hljs-string">registry.db</span>
<span class="hljs-attr">provider:</span> <span class="hljs-string">local</span>
<span class="hljs-attr">online_store:</span>
<span class="hljs-attr">    type:</span> <span class="hljs-string">sqlite</span>
<span class="hljs-attr">    path:</span> <span class="hljs-string">online_store.db</span>
<span class="hljs-attr">offline_store:</span>
<span class="hljs-attr">    type:</span> <span class="hljs-string">file</span>
<span class="hljs-attr">entity_key_serialization_version:</span> <span class="hljs-number">2</span>
</code></pre>
<p>The following top-level configuration options exist in the <code>feature_store.yaml</code> file.</p>
<ul>
<li>provider — Configures the environment in which Feast will deploy and operate.</li>
<li>registry — Configures the location of the feature registry.</li>
<li>online_store — Configures the online store.</li>
<li>offline_store — Configures the offline store.</li>
<li>project — Defines a namespace for the entire feature store. Can be used to isolate multiple deployments in a single installation of Feast. Should only contain letters, numbers, and underscores.</li>
</ul>
<!-- - engine - Configures the batch materialization engine. -->
<h3 id="3-Run-sample-workflow"><a class="anchor hidden-xs" href="#3-Run-sample-workflow" title="3-Run-sample-workflow"><i class="fa fa-link"></i></a>3. Run sample workflow</h3>
<ul>
<li>Prepare your data for loading to the feature store.</li>
</ul>
<pre><code class="python hljs">
<span class="hljs-comment"># Add two columns</span>
<span class="hljs-comment"># The timestamp and an id to reconize each row in the dataframe</span>

</code></pre>
<ul>
<li>Persist features to offline store</li>
</ul>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> feast <span class="hljs-keyword">import</span> (
    Entity, 
    FeatureView, 
    FileSource, 
    ValueType,
    FeatureService
)
<span class="hljs-keyword">from</span> feast.types <span class="hljs-keyword">import</span> Int32, Float32
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> init_hydra


BASE_PATH = os.path.expandvars(<span class="hljs-string">"$PROJECTPATH"</span>)

cfg = init_hydra()

PATH = cfg.features_path

entity_df = Entity(
    name=<span class="hljs-string">"bank_data"</span>,
    value_type=ValueType.INT32,
    description=<span class="hljs-string">"Identifier"</span>,
    join_keys=[<span class="hljs-string">'id'</span>]
)

source = FileSource(
                name=<span class="hljs-string">"bank_data_source"</span>,
                path = PATH,
                timestamp_field=<span class="hljs-string">"timestamp"</span>
                )


view = FeatureView(
    name = <span class="hljs-string">"bank_data_feature_views"</span>,
    entities=[entity_df],
    source=source,
    online=<span class="hljs-literal">False</span>,
    tags={},
    <span class="hljs-comment"># schema=</span>
)

<span class="hljs-comment"># This groups features into a feature service</span>
<span class="hljs-comment"># We will use it for versioning</span>
bank_records = FeatureService(
    name=<span class="hljs-string">"bank_data_features"</span> + <span class="hljs-string">"_"</span> + cfg.features_version,
    features=[
        view
    ],
)
</code></pre>
<ul>
<li>Retrieve features from offline store</li>
</ul>
<pre><code class="python hljs"><span class="hljs-comment"># services/feast/feast_repo/feature_repo/feast_repo.py</span>

<span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> init_hydra

cfg = init_hydra()
REPO_PATH = cfg.feature_store_path


store = FeatureStore(repo_path=REPO_PATH)

entity_df = Entity(
    name=<span class="hljs-string">"bank_data"</span>,
    value_type=ValueType.INT32,
    description=<span class="hljs-string">"Identifier"</span>,
    join_keys=[<span class="hljs-string">'id'</span>]
)

training_df = store.get_historical_features(
    entity_df=entity_df,
).to_df()

print(<span class="hljs-string">"----- Feature schema -----\n"</span>)
print(training_df.info())

print()
print(<span class="hljs-string">"----- Example features -----\n"</span>)
print(training_df.head())
</code></pre>
<ul>
<li>Register feature definitions and deploy your feature store</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-comment"># services/feast/feast_repo/feature_repo/test_workflow.py</span>

<span class="hljs-comment"># Make sure that you access the feature repo folder</span>

<span class="hljs-comment"># cd services/feast/feast_project/feature_repo/</span>
<span class="hljs-comment"># feast apply</span>
<span class="hljs-comment"># OR</span>
<span class="hljs-string">feast</span> <span class="hljs-bullet">-c</span> <span class="hljs-string">services/feast/feast_project/feature_repo</span> <span class="hljs-string">apply</span>
</code></pre>
<h3 id="4-Browse-your-features-with-the-Feast-Web-UI"><a class="anchor hidden-xs" href="#4-Browse-your-features-with-the-Feast-Web-UI" title="4-Browse-your-features-with-the-Feast-Web-UI"><i class="fa fa-link"></i></a>4. Browse your features with the Feast Web UI</h3>
<p>You can access the Feast Web UI via running the following command:</p>
<pre><code class="yaml hljs"><span class="hljs-string">feast</span> <span class="hljs-bullet">-c</span> <span class="hljs-string">services/feast/feast_project/feature_repo</span> <span class="hljs-string">ui</span>
</code></pre>
<p><img src="https://i.imgur.com/3wJLiJM.png" alt="" class="md-image md-image"></p>
<p>The server by default runs on port 8888.</p>
</details><h1 id="Project-tasks"><a class="anchor hidden-xs" href="#Project-tasks" title="Project-tasks"><i class="fa fa-link"></i></a>Project tasks</h1><p><strong>Note:</strong> The project tasks are graded, and they form the practice part of the course. We have tasks for repository and as well as for report (for Master’s student).</p><h2 id="A-Repository"><a class="anchor hidden-xs" href="#A-Repository" title="A-Repository"><i class="fa fa-link"></i></a>A. Repository</h2><div class="alert alert-success">
<ol>
<li>Build an automated workflow (<code>pipelines/data_extract_dag.py</code>) in Apache <strong>Airflow</strong> to perform the following 4 tasks. The workflow should be scheduled to run every 5 minutes (increase the time if your DAG run takes more than 5 minutes). This pipeline should be <strong>atomic</strong> such that if a single task/step fails, then all of the tasks/steps fail (default case in Apache Airflow).
<ul>
<li>Extract a new sample of the data.
<ul>
<li>You can use the functions you defined in phase 1.</li>
</ul>
</li>
<li>Validate the sample using Great Expectations.
<ul>
<li>You can use the functions you defined in phase 1.</li>
</ul>
</li>
<li>Version the sample using dvc.
<ul>
<li>You can use the scripts you defined in phase 1.</li>
<li>keep the version number of this data in a file <code>./configs/data_version.yaml</code>.</li>
</ul>
</li>
<li>Load the sample to the data store (remote storage of dvc repository in the local filesystem).
<ul>
<li>You can use the scripts you defined in phase 1.</li>
</ul>
</li>
</ul>
</li>
<li>Build an ETL pipeline (<code>pipelines/data_prepare.py</code>) using <strong>ZenML</strong>. The data pipeline consists of 4 tasks as follows:
<ul>
<li>Extract the data sample from the data store.
<ul>
<li>Write a function <code>read_datastore</code> in <code>src/data.py</code> which returns the sample as a dataframe/tensor.</li>
<li>Get the version number of this data from a file  <code>./configs/data_version.yaml</code>.</li>
<li>Send to the next step, the dataframe and data version.</li>
</ul>
</li>
<li>Transform the data sample into features.
<ul>
<li>Write a function <code>preprocess_data</code> in <code>src/data.py</code> which transforms the input data sample into features and returns them as a dataframe/tensor.</li>
<li>Send to the next step, the dataframe (X, y).</li>
</ul>
</li>
<li>Validate the features.
<ul>
<li>Build a new expectation suite for the features.</li>
<li>Create expectations to validate all features. One expectation type applied on one or more features but we should run expecatations on all features. For example, you can expect one hot encoding columns to be 0 or 1.</li>
<li>Create a batch request where the data asset is a pandas dataframe. Do not forget that here the pipeline tasks are exchanging data and not only metadata.</li>
<li>Create a checkpoint to check the validitiy of the features.</li>
<li>Write a function <code>validate_features</code> in <code>src/data.py</code> to validate the features by running the checkpoint.</li>
<li>Send to the next step, the dataframe (X, y).</li>
</ul>
</li>
<li>Load the features.
<ul>
<li>Create a function <code>load_features</code> in <code>src/data.py</code> to do as follows.
<ul>
<li>Load and version the features X and the target y in artifact store of ZenML.</li>
<li>Use <code>zenml.save_artifact</code> method with a name like <code>features_target</code> or <code>X_y</code> and same version as the data sample version. Here we will use the custom versions as tags, and let ZenML automatically increments the version.<pre><code class="python hljs"><span class="hljs-keyword">import</span> zenml

<span class="hljs-comment"># save the artifact as follows:</span>
<span class="hljs-comment"># df is your dataframe</span>
<span class="hljs-comment"># name is the artifact name</span>
<span class="hljs-comment"># version is your custom version (set it to tags)</span>
<span class="hljs-comment"># I did not set `version` argument since I want an automatic versioning</span>
zenml.save_artifact(data = df, name = <span class="hljs-string">"features_target"</span>, tags=[version])

<span class="hljs-keyword">from</span> zenml.client <span class="hljs-keyword">import</span> Client
client = Client()

<span class="hljs-comment"># We can retrieve our artifact with a specific custom version `v5` as follows:</span>
l = client.list_artifact_versions(name=<span class="hljs-string">"features_target"</span>, tag=<span class="hljs-string">"v5"</span>, sort_by=<span class="hljs-string">"version"</span>).items

<span class="hljs-comment"># Descending order</span>
l.reverse()

<span class="hljs-comment"># l here is a list of items and l[0] will retrieve the latest version of the artifact</span>
df = l[<span class="hljs-number">0</span>].load() <span class="hljs-comment"># pd.DataFrame</span>
</code></pre>
</li>
</ul>
</li>
<li>Ensure that you can retrieve your version from ZenML store. (Use <code>zenml.load_artifact</code>)</li>
<li><strong>[optional task related to Feast]</strong> Persist the features (X, y) in a parquet file <code>data/processed/features.parquet</code>.</li>
</ul>
</li>
</ul>
</li>
<li>Create a DAG (<code>pipelines/data_prepare_dag.py</code>) to run the previous ZenML pipeline when all the tasks in data extraction pipeline (<code>pipelines/data_extract_dag.py</code>) are successful (use <code>ExternalTaskSensor</code>) and then (load the features to a feature store <em>if you prefer</em> (optional)). If the first pipeline is failed then we should not run the second pipeline. This DAG will ensure the automataion of data preparation. The DAG should be scheduled to run every 5 minutes (same schedule as first pipeline). This pipeline should be <strong>atomic</strong> such that if a single task/step fails, then all of the tasks/steps fail. The tasks here are:
<ul>
<li>An external task sensor to wait for completion of first pipeline (data extraction.</li>
<li>A Bash task to run the ZenML pipeline.</li>
<li><strong>[Optional task]</strong> Load the features to a feature store (parquet file as a data source).
<ul>
<li>Create a file source and read the features from the parquet file.</li>
<li>Add <code>timestamp</code>, <code>id</code> fields to the dataframe (X, y). If your data has <code>timestamp</code> field, then you can use it.</li>
<li>Create an offline feature view with defined schema.</li>
<li>Create a feature service to retrieve all features.</li>
<li>Check if you can retrieve features from the offline store.<br>
<strong>Note:</strong> Notice that this source code should be added to the folder where feature repo is initialized (e.g. <code>services/feast/feast_project/feature_repo/bank_repo.py</code> in my case) and you can run this source code by executing a command line as follows:</li>
</ul>
<pre><code class="yaml hljs"><span class="hljs-string">feast</span> <span class="hljs-bullet">-c</span> <span class="hljs-string">services/feast/feast_project/feature_repo</span> <span class="hljs-string">apply</span>
</code></pre>
<strong>Note:</strong> You can visualize the change in the feature repo by going to the feast dashboard:<pre><code class="yaml hljs"><span class="hljs-string">feast</span> <span class="hljs-bullet">-c</span> <span class="hljs-string">services/feast/feast_project/feature_repo</span> <span class="hljs-string">ui</span>
</code></pre>
</li>
</ul>
</li>
<li>For each function/method you wrote/defined in <code>src</code> folder, write at least one test function in <code>tests</code> folder and test your modules/classes/functions.</li>
<li>Make sure that you push your changes to Github for submission.</li>
</ol>
</div><div class="alert alert-danger">
<p><strong>Important note:</strong><br>
Do not write the business logic code in dag definition files in <code>pipelines</code> folder. You should write them in <code>src</code> folder in <code>src/data.py</code> module, then you can call them from <code>pipelines</code> folder when you create dags. This is true for all pipelines (Airflow and ZenML). You should introduce the code in <code>src</code> for regular testing (<code>pytest</code>). You can test pipelines using <code>airflow dags test</code> subcommand.</p>
</div><div class="alert alert-warning">
<p><strong>Info:</strong> ZenML actually versions the artifacts and store them in a local store in the file system. So for this project, we do not need to use a special feature store as it will add more unnecessary complexity. It is good to mention that Feast does not have a versioning feature and you need to do so using other tools like <code>dvc</code> whereas other feature store tools such as <a href="https://www.featureform.com/post/tutorial-feature-versioning-101" target="_blank" rel="noopener">featureform</a> may support versioning but we will not cover it in this project.</p>
</div><h2 id="B-Report-Only-for-Master’s-students"><a class="anchor hidden-xs" href="#B-Report-Only-for-Master’s-students" title="B-Report-Only-for-Master’s-students"><i class="fa fa-link"></i></a>B. Report <strong>[Only for Master’s students]</strong></h2><div class="alert alert-success">
<p>Complete the following chapters:</p>
<ul>
<li><strong>Chapter 3: Data preparation</strong>
<ul>
<li>The data preparation phase covers all activities to construct the final dataset (data that will be fed into the machine learning pipelines) from the initial raw data. Data preparation tasks are likely to be performed multiple times and not in any prescribed order. Tasks include table, record and attribute selection as well as transformation and cleaning of data for modeling phase.</li>
<li><strong>Section 3.1. Select data</strong>
<ul>
<li>Decide on the data to be used for analysis. Criteria include relevance to the machine learning goals, quality and technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table.</li>
<li>List the data to be included/excluded and the reasons for these decisions.</li>
</ul>
</li>
<li><strong>Section 3.2. Clean data</strong>
<ul>
<li>Raise the data quality to the level required by the selected machine learning techniques. This may involve selection of clean subsets of the data, the insertion of suitable defaults or more ambitious techniques such as the estimation of missing data by modeling.</li>
<li>Describe what decisions and actions were taken to address the data quality problems reported during the verify data quality task of the business and data understanding phase. Transformations of the data for cleaning purposes and the possible impact on the analysis results should be considered.</li>
</ul>
</li>
<li><strong>Section 3.3. Construct data</strong>
<ul>
<li>This task includes constructive data preparation operations such as the production of derived attributes, entire new records or transformed values for existing attributes.</li>
<li>Derived attributes are new attributes that are constructed from one or more existing attributes in the same record.</li>
<li>Describe the creation of completely new records.
<ul>
<li>Example: create records for customers who made no purchase during the past year.There was no reason to have such records in the raw data, but for modeling purposes it might make sense to explicitly represent the fact that certain customers made zero purchases.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Section 3.4. Inegrate data</strong>
<ul>
<li>You can omit this section due to using only one data source.</li>
</ul>
</li>
<li><strong>Section 3.5. Standardize data</strong>
<ul>
<li>Describe normalization methods used to unify the data. Do not forget that the data fed to the modeling stage is a single object.</li>
<li>Merge data containers if you have more than one.</li>
<li>Describe how you develop a uniform schema for your dataset</li>
</ul>
</li>
</ul>
</li>
</ul>
</div><h1 id="References"><a class="anchor hidden-xs" href="#References" title="References"><i class="fa fa-link"></i></a>References</h1><ul>
<li><a href="https://ml-ops.org/content/crisp-ml" target="_blank" rel="noopener">CRISP-ML(Q)</a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/index.html" target="_blank" rel="noopener">Airflow docs</a></li>
<li><a href="https://canonical-ubuntu-wsl.readthedocs-hosted.com/en/latest/guides/install-ubuntu-wsl2/" target="_blank" rel="noopener">Install Ubuntu on WSL2</a></li>
<li><a href="https://docs.zenml.io/" target="_blank" rel="noopener">ZenML docs</a></li>
<li><a href="https://docs.feast.dev/" target="_blank" rel="noopener">Feast docs</a></li>
<li><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker docs</a></li>
<li><a href="https://www.astronomer.io/docs/learn" target="_blank" rel="noopener">Astronomer docs</a></li>
<li><a href="https://docs.vultr.com/how-to-deploy-apache-airflow-on-ubuntu-20-04" target="_blank" rel="noopener">How to Deploy Apache Airflow on Ubuntu</a></li>
</ul></div>
    <div class="ui-toc dropup unselectable hidden-print" style="display:none;">
        <div class="pull-right dropdown">
            <a id="tocLabel" class="ui-toc-label btn btn-default" data-toggle="dropdown" href="#" role="button" aria-haspopup="true" aria-expanded="false" title="Table of content">
                <i class="fa fa-bars"></i>
            </a>
            <ul id="ui-toc" class="ui-toc-dropdown dropdown-menu" aria-labelledby="tocLabel">
                <div class="toc"><ul class="nav">
<li><a href="#Phase-II---Data-preparationengineering" title="Phase II - Data preparation/engineering">Phase II - Data preparation/engineering</a></li>
<li><a href="#Dataset" title="Dataset">Dataset</a></li>
<li><a href="#Agenda" title="Agenda">Agenda</a></li>
<li><a href="#Description" title="Description">Description</a></li>
<li><a href="#Docker-and-Docker-compose-Extra-section-for-now" title="Docker and Docker compose [Extra section for now]">Docker and Docker compose [Extra section for now]</a><ul class="nav">
<li><a href="#Dockerfile-definition-extra-section" title="Dockerfile definition [extra section]">Dockerfile definition [extra section]</a></li>
</ul>
</li>
<li class=""><a href="#Apache-Airflow" title="Apache Airflow">Apache Airflow</a><ul class="nav">
<li><a href="#Install-Airflow" title="Install Airflow">Install Airflow</a><ul class="nav">
<li><a href="#1-Usingpip" title="1. Usingpip">1. Usingpip</a></li>
<li><a href="#2-Using-Docker" title="2. Using Docker">2. Using Docker</a></li>
</ul>
</li>
<li><a href="#Prepare-the-workspace-tested-on-Ubuntu-2204" title="Prepare the workspace [tested on Ubuntu 22.04]">Prepare the workspace [tested on Ubuntu 22.04]</a><ul class="nav">
<li><a href="#1-Install-python311" title="1. Install python3.11">1. Install python3.11</a></li>
<li><a href="#2-Create-a-new-virtual-environment-using-Python-311" title="2. Create a new virtual environment using Python 3.11">2. Create a new virtual environment using Python 3.11</a></li>
<li><a href="#3-Create-requirementstxt-and-install-the-dependencies" title="3. Create requirements.txt and install the dependencies.">3. Create requirements.txt and install the dependencies.</a></li>
<li><a href="#4-Setup-the-Airflow-components" title="4. Setup the Airflow components">4. Setup the Airflow components</a></li>
</ul>
</li>
<li><a href="#Hello-world-workflow" title="Hello world workflow">Hello world workflow</a></li>
<li><a href="#Core-concepts" title="Core concepts">Core concepts</a></li>
<li><a href="#Airflow-components" title="Airflow components">Airflow components</a></li>
<li><a href="#DAG-Scheduling" title="DAG Scheduling">DAG Scheduling</a><ul class="nav">
<li><a href="#Scheduling-concepts" title="Scheduling concepts">Scheduling concepts</a></li>
</ul>
</li>
<li class=""><a href="#Airflow-DAG-APIs" title="Airflow DAG APIs">Airflow DAG APIs</a></li>
<li class=""><a href="#DAG-Examples" title="DAG Examples">DAG Examples</a><ul class="nav">
<li class=""><a href="#1-Traditional-API-as-a-variable" title="1. Traditional API as a variable">1. Traditional API as a variable</a></li>
<li class=""><a href="#2-Traditional-API-as-a-context" title="2. Traditional API as a context">2. Traditional API as a context</a></li>
<li class=""><a href="#3-TaskFlow-API" title="3. TaskFlow API">3. TaskFlow API</a></li>
<li class=""><a href="#Xcoms-extra-section" title="Xcoms [extra section]">Xcoms [extra section]</a></li>
<li class=""><a href="#Python-tasks-with-virtual-environments" title="Python tasks with virtual environments">Python tasks with virtual environments</a></li>
<li class=""><a href="#ExternalTaskSensor" title="ExternalTaskSensor">ExternalTaskSensor</a></li>
<li class=""><a href="#Trigger-Dag-Run-operator" title="Trigger Dag Run operator">Trigger Dag Run operator</a></li>
</ul>
</li>
<li class=""><a href="#Deferrable-operators-Extra-section" title="Deferrable operators [Extra section]">Deferrable operators [Extra section]</a></li>
<li class=""><a href="#Test-a-pipeline" title="Test a pipeline">Test a pipeline</a></li>
<li class=""><a href="#Airflow-UI" title="Airflow UI">Airflow UI</a><ul class="nav">
<li><a href="#DAGs-View" title="DAGs View">DAGs View</a></li>
<li><a href="#Cluster-Activity-View" title="Cluster Activity View">Cluster Activity View</a></li>
<li><a href="#Datasets-View" title="Datasets View">Datasets View</a></li>
<li><a href="#Grid-View" title="Grid View">Grid View</a></li>
<li><a href="#Graph-View" title="Graph View">Graph View</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#ZenML" title="ZenML">ZenML</a><ul class="nav">
<li><a href="#Core-concepts1" title="Core concepts">Core concepts</a><ul class="nav">
<li><a href="#Step" title="Step">Step</a></li>
<li><a href="#Pipelines" title="Pipelines">Pipelines</a></li>
<li><a href="#Artifacts" title="Artifacts">Artifacts</a></li>
<li><a href="#Materializers" title="Materializers">Materializers</a></li>
</ul>
</li>
<li><a href="#ZenML-Example" title="ZenML Example">ZenML Example</a><ul class="nav">
<li><a href="#ETL-data-pipeline" title="ETL data pipeline">ETL data pipeline</a></li>
</ul>
</li>
<li><a href="#Explore-ZenML-Dashboard" title="Explore ZenML Dashboard">Explore ZenML Dashboard</a></li>
<li><a href="#ZenML-Stacks" title="ZenML Stacks">ZenML Stacks</a><ul class="nav">
<li><a href="#Orchestrator" title="Orchestrator">Orchestrator</a></li>
<li><a href="#Artifact-store" title="Artifact store">Artifact store</a></li>
<li><a href="#Create-a-local-stack-extra-section" title="Create a local stack [extra section]">Create a local stack [extra section]</a></li>
</ul>
</li>
<li><a href="#Manage-artifacts-in-ZenML" title="Manage artifacts in ZenML">Manage artifacts in ZenML</a><ul class="nav">
<li><a href="#Adding-artifacts" title="Adding artifacts">Adding artifacts</a></li>
<li><a href="#Consuming-artifacts" title="Consuming artifacts">Consuming artifacts</a></li>
</ul>
</li>
<li><a href="#Schedule-ZenML-pipelines-using-Airflow" title="Schedule ZenML pipelines using Airflow">Schedule ZenML pipelines using Airflow</a></li>
</ul>
</li>
<li><a href="#Feast-Extra-section" title="Feast [Extra section]">Feast [Extra section]</a></li>
<li><a href="#Project-tasks" title="Project tasks">Project tasks</a><ul class="nav">
<li><a href="#A-Repository" title="A. Repository">A. Repository</a></li>
<li><a href="#B-Report-Only-for-Master’s-students" title="B. Report [Only for Master’s students]">B. Report [Only for Master’s students]</a></li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><div class="toc-menu" style="">
    <a class="expand-toggle expand-all" href="#">Expand all</a>
    <a class="expand-toggle collapse-all" href="#" style="display: none;">Collapse all</a>
    <a class="back-to-top" href="#">Back to top</a>
    <a class="go-to-bottom" href="#">Go to bottom</a>
</div>
            </ul>
        </div>
    </div>
    <div id="ui-toc-affix" class="ui-affix-toc ui-toc-dropdown unselectable hidden-print" data-spy="affix" style="top:17px;display:none;"  >
        <div class="toc"><ul class="nav">
<li><a href="#Phase-II---Data-preparationengineering" title="Phase II - Data preparation/engineering">Phase II - Data preparation/engineering</a></li>
<li><a href="#Dataset" title="Dataset">Dataset</a></li>
<li><a href="#Agenda" title="Agenda">Agenda</a></li>
<li><a href="#Description" title="Description">Description</a></li>
<li><a href="#Docker-and-Docker-compose-Extra-section-for-now" title="Docker and Docker compose [Extra section for now]">Docker and Docker compose [Extra section for now]</a><ul class="nav">
<li><a href="#Dockerfile-definition-extra-section" title="Dockerfile definition [extra section]">Dockerfile definition [extra section]</a></li>
</ul>
</li>
<li class=""><a href="#Apache-Airflow" title="Apache Airflow">Apache Airflow</a><ul class="nav">
<li><a href="#Install-Airflow" title="Install Airflow">Install Airflow</a><ul class="nav">
<li><a href="#1-Usingpip" title="1. Usingpip">1. Usingpip</a></li>
<li><a href="#2-Using-Docker" title="2. Using Docker">2. Using Docker</a></li>
</ul>
</li>
<li><a href="#Prepare-the-workspace-tested-on-Ubuntu-2204" title="Prepare the workspace [tested on Ubuntu 22.04]">Prepare the workspace [tested on Ubuntu 22.04]</a><ul class="nav">
<li><a href="#1-Install-python311" title="1. Install python3.11">1. Install python3.11</a></li>
<li><a href="#2-Create-a-new-virtual-environment-using-Python-311" title="2. Create a new virtual environment using Python 3.11">2. Create a new virtual environment using Python 3.11</a></li>
<li><a href="#3-Create-requirementstxt-and-install-the-dependencies" title="3. Create requirements.txt and install the dependencies.">3. Create requirements.txt and install the dependencies.</a></li>
<li><a href="#4-Setup-the-Airflow-components" title="4. Setup the Airflow components">4. Setup the Airflow components</a></li>
</ul>
</li>
<li><a href="#Hello-world-workflow" title="Hello world workflow">Hello world workflow</a></li>
<li><a href="#Core-concepts" title="Core concepts">Core concepts</a></li>
<li><a href="#Airflow-components" title="Airflow components">Airflow components</a></li>
<li><a href="#DAG-Scheduling" title="DAG Scheduling">DAG Scheduling</a><ul class="nav">
<li><a href="#Scheduling-concepts" title="Scheduling concepts">Scheduling concepts</a></li>
</ul>
</li>
<li class=""><a href="#Airflow-DAG-APIs" title="Airflow DAG APIs">Airflow DAG APIs</a></li>
<li class=""><a href="#DAG-Examples" title="DAG Examples">DAG Examples</a><ul class="nav">
<li class=""><a href="#1-Traditional-API-as-a-variable" title="1. Traditional API as a variable">1. Traditional API as a variable</a></li>
<li class=""><a href="#2-Traditional-API-as-a-context" title="2. Traditional API as a context">2. Traditional API as a context</a></li>
<li class=""><a href="#3-TaskFlow-API" title="3. TaskFlow API">3. TaskFlow API</a></li>
<li class=""><a href="#Xcoms-extra-section" title="Xcoms [extra section]">Xcoms [extra section]</a></li>
<li class=""><a href="#Python-tasks-with-virtual-environments" title="Python tasks with virtual environments">Python tasks with virtual environments</a></li>
<li class=""><a href="#ExternalTaskSensor" title="ExternalTaskSensor">ExternalTaskSensor</a></li>
<li class=""><a href="#Trigger-Dag-Run-operator" title="Trigger Dag Run operator">Trigger Dag Run operator</a></li>
</ul>
</li>
<li class=""><a href="#Deferrable-operators-Extra-section" title="Deferrable operators [Extra section]">Deferrable operators [Extra section]</a></li>
<li class=""><a href="#Test-a-pipeline" title="Test a pipeline">Test a pipeline</a></li>
<li class=""><a href="#Airflow-UI" title="Airflow UI">Airflow UI</a><ul class="nav">
<li><a href="#DAGs-View" title="DAGs View">DAGs View</a></li>
<li><a href="#Cluster-Activity-View" title="Cluster Activity View">Cluster Activity View</a></li>
<li><a href="#Datasets-View" title="Datasets View">Datasets View</a></li>
<li><a href="#Grid-View" title="Grid View">Grid View</a></li>
<li><a href="#Graph-View" title="Graph View">Graph View</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#ZenML" title="ZenML">ZenML</a><ul class="nav">
<li><a href="#Core-concepts1" title="Core concepts">Core concepts</a><ul class="nav">
<li><a href="#Step" title="Step">Step</a></li>
<li><a href="#Pipelines" title="Pipelines">Pipelines</a></li>
<li><a href="#Artifacts" title="Artifacts">Artifacts</a></li>
<li><a href="#Materializers" title="Materializers">Materializers</a></li>
</ul>
</li>
<li><a href="#ZenML-Example" title="ZenML Example">ZenML Example</a><ul class="nav">
<li><a href="#ETL-data-pipeline" title="ETL data pipeline">ETL data pipeline</a></li>
</ul>
</li>
<li><a href="#Explore-ZenML-Dashboard" title="Explore ZenML Dashboard">Explore ZenML Dashboard</a></li>
<li><a href="#ZenML-Stacks" title="ZenML Stacks">ZenML Stacks</a><ul class="nav">
<li><a href="#Orchestrator" title="Orchestrator">Orchestrator</a></li>
<li><a href="#Artifact-store" title="Artifact store">Artifact store</a></li>
<li><a href="#Create-a-local-stack-extra-section" title="Create a local stack [extra section]">Create a local stack [extra section]</a></li>
</ul>
</li>
<li><a href="#Manage-artifacts-in-ZenML" title="Manage artifacts in ZenML">Manage artifacts in ZenML</a><ul class="nav">
<li><a href="#Adding-artifacts" title="Adding artifacts">Adding artifacts</a></li>
<li><a href="#Consuming-artifacts" title="Consuming artifacts">Consuming artifacts</a></li>
</ul>
</li>
<li><a href="#Schedule-ZenML-pipelines-using-Airflow" title="Schedule ZenML pipelines using Airflow">Schedule ZenML pipelines using Airflow</a></li>
</ul>
</li>
<li><a href="#Feast-Extra-section" title="Feast [Extra section]">Feast [Extra section]</a></li>
<li><a href="#Project-tasks" title="Project tasks">Project tasks</a><ul class="nav">
<li><a href="#A-Repository" title="A. Repository">A. Repository</a></li>
<li><a href="#B-Report-Only-for-Master’s-students" title="B. Report [Only for Master’s students]">B. Report [Only for Master’s students]</a></li>
</ul>
</li>
<li><a href="#References" title="References">References</a></li>
</ul>
</div><div class="toc-menu" style="">
    <a class="expand-toggle expand-all" href="#">Expand all</a>
    <a class="expand-toggle collapse-all" href="#" style="display: none;">Collapse all</a>
    <a class="back-to-top" href="#">Back to top</a>
    <a class="go-to-bottom" href="#">Go to bottom</a>
</div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.0/js/bootstrap.min.js" integrity="sha256-kJrlY+s09+QoWjpkOrXXwhxeaoDz9FW5SaxF8I0DibQ=" crossorigin="anonymous" defer></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gist-embed/2.6.0/gist-embed.min.js" integrity="sha256-KyF2D6xPIJUW5sUDSs93vWyZm+1RzIpKCexxElmxl8g=" crossorigin="anonymous" defer></script>
    <script>
        var markdown = $(".markdown-body");
        //smooth all hash trigger scrolling
        function smoothHashScroll() {
            var hashElements = $("a[href^='#']").toArray();
            for (var i = 0; i < hashElements.length; i++) {
                var element = hashElements[i];
                var $element = $(element);
                var hash = element.hash;
                if (hash) {
                    $element.on('click', function (e) {
                        // store hash
                        var hash = this.hash;
                        if ($(hash).length <= 0) return;
                        // prevent default anchor click behavior
                        e.preventDefault();
                        // animate
                        $('body, html').stop(true, true).animate({
                            scrollTop: $(hash).offset().top
                        }, 100, "linear", function () {
                            // when done, add hash to url
                            // (default click behaviour)
                            window.location.hash = hash;
                        });
                    });
                }
            }
        }

        smoothHashScroll();
        var toc = $('.ui-toc');
        var tocAffix = $('.ui-affix-toc');
        var tocDropdown = $('.ui-toc-dropdown');
        //toc
        tocDropdown.click(function (e) {
            e.stopPropagation();
        });

        var enoughForAffixToc = true;

        function generateScrollspy() {
            $(document.body).scrollspy({
                target: ''
            });
            $(document.body).scrollspy('refresh');
            if (enoughForAffixToc) {
                toc.hide();
                tocAffix.show();
            } else {
                tocAffix.hide();
                toc.show();
            }
            $(document.body).scroll();
        }

        function windowResize() {
            //toc right
            var paddingRight = parseFloat(markdown.css('padding-right'));
            var right = ($(window).width() - (markdown.offset().left + markdown.outerWidth() - paddingRight));
            toc.css('right', right + 'px');
            //affix toc left
            var newbool;
            var rightMargin = (markdown.parent().outerWidth() - markdown.outerWidth()) / 2;
            //for ipad or wider device
            if (rightMargin >= 133) {
                newbool = true;
                var affixLeftMargin = (tocAffix.outerWidth() - tocAffix.width()) / 2;
                var left = markdown.offset().left + markdown.outerWidth() - affixLeftMargin;
                tocAffix.css('left', left + 'px');
            } else {
                newbool = false;
            }
            if (newbool != enoughForAffixToc) {
                enoughForAffixToc = newbool;
                generateScrollspy();
            }
        }
        $(window).resize(function () {
            windowResize();
        });
        $(document).ready(function () {
            windowResize();
            generateScrollspy();
        });

        //remove hash
        function removeHash() {
            window.location.hash = '';
        }

        var backtotop = $('.back-to-top');
        var gotobottom = $('.go-to-bottom');

        backtotop.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToTop)
                scrollToTop();
            removeHash();
        });
        gotobottom.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            if (scrollToBottom)
                scrollToBottom();
            removeHash();
        });

        var toggle = $('.expand-toggle');
        var tocExpand = false;

        checkExpandToggle();
        toggle.click(function (e) {
            e.preventDefault();
            e.stopPropagation();
            tocExpand = !tocExpand;
            checkExpandToggle();
        })

        function checkExpandToggle () {
            var toc = $('.ui-toc-dropdown .toc');
            var toggle = $('.expand-toggle');
            if (!tocExpand) {
                toc.removeClass('expand');
                toggle.text('Expand all');
            } else {
                toc.addClass('expand');
                toggle.text('Collapse all');
            }
        }

        function scrollToTop() {
            $('body, html').stop(true, true).animate({
                scrollTop: 0
            }, 100, "linear");
        }

        function scrollToBottom() {
            $('body, html').stop(true, true).animate({
                scrollTop: $(document.body)[0].scrollHeight
            }, 100, "linear");
        }
    </script>
</body>

</html>
